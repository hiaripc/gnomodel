{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 12:00:56.499 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2025-02-05 12:00:56.566 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2025-02-05 12:00:56.567 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2025-02-05 12:00:56.568 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-05 12:00:56.569 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2025-02-05 12:00:56.571 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2025-02-05 12:00:56.571 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2025-02-05 12:00:56.572 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2025-02-05 12:00:56.572 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2025-02-05 12:00:56.573 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2025-02-05 12:00:56.573 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2025-02-05 12:00:56.574 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2025-02-05 12:00:56.575 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2025-02-05 12:00:56.575 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2025-02-05 12:00:56.576 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2025-02-05 12:00:56.577 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2025-02-05 12:00:56.580 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv_transpose2d be migrated to C++?\n",
      "2025-02-05 12:00:56.582 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-05 12:00:56.583 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ee19ef2f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "import random\n",
    "from models.utility_functions import comp_pcc\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=64, n_layers=5, n_heads=8, n_kv_heads=4, vocab_size=512, hidden_dim=None, multiple_of=4, norm_eps=1e-05, max_seq_len=512, dropout=0.5)\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import precompute_freqs_cis\n",
    "\n",
    "# Load checkpoint dict\n",
    "checkpoint_dict = torch.load(\"llama2/configs/stories260K.pth\")\n",
    "model_args = checkpoint_dict['model_args']\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "args = ModelArgs()\n",
    "print(args)\n",
    "\n",
    "torch_freqs_cos, torch_freqs_sin = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "freqs_cos = torch_freqs_cos.bfloat16()\n",
    "freqs_sin = torch_freqs_sin.bfloat16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokens_num = args.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2025-02-05 12:00:56.749\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0, IOMMU: disabled\n",
      "\u001b[32m2025-02-05 12:00:56.750\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-02-05 12:00:56.750\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "# device.enable_program_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(logits, temperature, top_k):\n",
    "    logits = logits[:, -1, :] # crop to just the final time step\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        # \"sample\" the single most likely index\n",
    "        _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "    else:\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = torch.functional.F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.tokenizer import Tokenizer\n",
    "\n",
    "enc = Tokenizer(tokenizel_path=\"./llama2/tokenizer.model\")\n",
    "temperature = 1.0\n",
    "top_k = 300\n",
    "token_to_generate = 100\n",
    "\n",
    "start = \"Dake was walking\"\n",
    "start_ids = enc.encode(start, bos=True, eos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTNN llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:13<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dake was walking in Wain. judge was fast and she hurt herself. She left a mysterious cone, a man with a vase.\n",
      "\"Be carefully, Nappy!\" he said.\n",
      "\"Please, please hope this is a napk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ttllama2 import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tt_trans = Transformer(args, state_dict, device)\n",
    "\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "first_token = 0\n",
    "time_forward = 0\n",
    "start_generation = time.time()\n",
    "\n",
    "for _ in tqdm(range(token_to_generate)):\n",
    "    \n",
    "    tt_x = ttnn.from_torch(x, device=device)\n",
    "    tt_x = ttnn.to_layout(tt_x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "    start_forward = time.time()\n",
    "    logits = tt_trans.forward(tt_x)\n",
    "    time_forward += time.time() - start_forward\n",
    "\n",
    "    if first_token == 0:\n",
    "        first_token = start_forward\n",
    "\n",
    "    logits = ttnn.to_torch(logits)\n",
    "    idx_next = get_next_token(logits, temperature, top_k)    \n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "    \n",
    "print(enc.decode(x[0].tolist()))\n",
    "\n",
    "tot_generation = time.time() - start_generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot generation time: 73.04002594947815\n",
      "Tot forward time: 72.6094172000885\n",
      "Time To First Token: 1738760259.4030492\n",
      "Tokens per Second: 1.3691123284809632\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tot generation time: {tot_generation}\")\n",
    "print(f\"Tot forward time: {time_forward}\")\n",
    "print(f\"Time To First Token: {first_token}\")\n",
    "print(f\"Tokens per Second: {token_to_generate / tot_generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream comes true! The duck loved to kick their favorite puppy. One day Diamble, a rich ant named Cuppy who went outside to play. Dino was very clapping and okay.\n",
      "Max's monkey gave him a big hug. Finally,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import Transformer as torchTransformer\n",
    "\n",
    "torch_trans = torchTransformer(args, state_dict)\n",
    "\n",
    "# [1,13]\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "first_token_torch = 0\n",
    "time_forward_torch = 0\n",
    "start_generation = time.time()\n",
    "\n",
    "for _ in tqdm(range(token_to_generate)):\n",
    "    # if the sequence context is growing too long we must crop it at block_size\n",
    "    # x = x if idx.size(1) <= args.max_seq_len else idx[:, -args.max_seq_len:]\n",
    "    \n",
    "    start_forward = time.time()\n",
    "    logits = torch_trans.forward(x)   \n",
    "    time_forward_torch += time.time() - start_forward\n",
    "\n",
    "    if first_token_torch == 0:\n",
    "        first_token_torch = start_forward\n",
    "    # forward the model to get the logits for the index in the sequence\n",
    "\n",
    "    idx_next = get_next_token(logits, temperature, top_k)\n",
    "    \n",
    "    # append sampled index to the running sequence and continue\n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "\n",
    "tot_generation_torch = time.time() - start_generation\n",
    "\n",
    "print(enc.decode(x[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot generation time: 2.2409582138061523\n",
      "Tot forward time: 2.187505006790161\n",
      "Time To First Token: 1738754933.197594\n",
      "Tokens per Second: 44.623768254096596\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tot generation time: {tot_generation_torch}\")\n",
    "print(f\"Tot forward time: {time_forward_torch}\")\n",
    "print(f\"Time To First Token: {first_token_torch}\")\n",
    "print(f\"Tokens per Second: {token_to_generate / tot_generation_torch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
