{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 15:48:10.972 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2025-02-14 15:48:11.047 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2025-02-14 15:48:11.048 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2025-02-14 15:48:11.049 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-14 15:48:11.051 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2025-02-14 15:48:11.051 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2025-02-14 15:48:11.052 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2025-02-14 15:48:11.053 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2025-02-14 15:48:11.053 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2025-02-14 15:48:11.054 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2025-02-14 15:48:11.055 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2025-02-14 15:48:11.055 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2025-02-14 15:48:11.056 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2025-02-14 15:48:11.056 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2025-02-14 15:48:11.057 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2025-02-14 15:48:11.057 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2025-02-14 15:48:11.061 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv_transpose2d be migrated to C++?\n",
      "2025-02-14 15:48:11.064 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-14 15:48:11.065 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff6c9ebb2f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "import random\n",
    "from models.utility_functions import comp_pcc\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=512, n_layers=8, n_heads=8, n_kv_heads=8, vocab_size=32000, hidden_dim=None, multiple_of=32, norm_eps=1e-05, max_seq_len=1024, dropout=0.5)\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import precompute_freqs_cis\n",
    "from llama2.tokenizer import Tokenizer\n",
    "\n",
    "# Load checkpoint dict\n",
    "checkpoint_dict = torch.load(\"llama2/configs/stories42M.pt\")\n",
    "enc = Tokenizer(tokenizel_path=\"./llama2/tokenizerM.model\")\n",
    "\n",
    "\n",
    "model_args = checkpoint_dict['model_args']\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "args = ModelArgs(**model_args)\n",
    "print(args)\n",
    "\n",
    "\n",
    "\n",
    "torch_freqs_cos, torch_freqs_sin = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "freqs_cos = torch_freqs_cos.bfloat16()\n",
    "freqs_sin = torch_freqs_sin.bfloat16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokens_num = args.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2025-02-14 15:48:11.302\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0, IOMMU: disabled\n",
      "\u001b[32m2025-02-14 15:48:11.303\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-02-14 15:48:11.303\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Enabling program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "device.enable_program_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(logits, temperature, top_k):\n",
    "    logits = logits[:, -1, :] # crop to just the final time step\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        # \"sample\" the single most likely index\n",
    "        _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "    else:\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = torch.functional.F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0\n",
    "top_k = 300\n",
    "token_to_generate = 100\n",
    "\n",
    "start = \"hiari is trying to make this damn thing work\"\n",
    "start_ids = enc.encode(start, bos=True, eos=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTNN llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttllama2 import Transformer\n",
    "\n",
    "tt_trans = Transformer(args, state_dict, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:23<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiari is trying to make this damn thing work. He had worked so hard but his mom was not very happy. She said it was too hard. So, his dad took him on an airplane to make it there.\n",
      "When they arrived, his dad took him in and got in the airplane. It was so much fun! He was amazed by the sky and the wind was blowing throughout his faces. Everything was so big and beautiful.\n",
      "When it was over, his dad handed him a small hammer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "first_token = 0\n",
    "time_forward = 0\n",
    "start_generation = time.time()\n",
    "\n",
    "for _ in tqdm(range(token_to_generate)):\n",
    "    \n",
    "    x = x if x.size(1) <= args.max_seq_len else x[:, -args.max_seq_len:]\n",
    "\n",
    "    tt_x = ttnn.from_torch(x, device=device)\n",
    "    tt_x = ttnn.to_layout(tt_x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "    start_forward = time.time()\n",
    "    logits = tt_trans.forward(tt_x)\n",
    "    time_forward += time.time() - start_forward\n",
    "\n",
    "    if first_token == 0:\n",
    "        first_token = time_forward\n",
    "\n",
    "    logits = ttnn.to_torch(logits)\n",
    "    idx_next = get_next_token(logits, temperature, top_k)    \n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "    \n",
    "print(enc.decode(x[0].tolist()))\n",
    "\n",
    "tot_generation = time.time() - start_generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot generation time: 143.449\n",
      "Tot forward time: 136.042\n",
      "Time To First Token: 10.972\n",
      "Tokens per Second: 0.697\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tot generation time: {tot_generation:.3f}\")\n",
    "print(f\"Tot forward time: {time_forward:.3f}\")\n",
    "print(f\"Time To First Token: {first_token:.3f}\")\n",
    "print(f\"Tokens per Second: {token_to_generate / tot_generation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:17<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiari is trying to make this damn thing work all her others.\n",
      "She wasnby the and a,inter still playing. She had so long, little in each place.\n",
      "One. particular was not a fierce fire, the can was the M true.\n",
      "She leaned down to the first, then the night, in the back savally howling, afraid to find himself. Come. In your jail or your friends were und with such the preby of nature of a novel. 10 roo?\"\n",
      "H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import Transformer as torchTransformer\n",
    "\n",
    "torch_trans = torchTransformer(args, state_dict)\n",
    "\n",
    "# [1,13]\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "first_token_torch = 0\n",
    "time_forward_torch = 0\n",
    "start_generation = time.time()\n",
    "\n",
    "for _ in tqdm(range(token_to_generate)):\n",
    "    # if the sequence context is growing too long we must crop it at block_size\n",
    "    x = x if x.size(1) <= args.max_seq_len else x[:, -args.max_seq_len:]\n",
    "    \n",
    "    start_forward = time.time()\n",
    "    logits = torch_trans.forward(x)   \n",
    "    time_forward_torch += time.time() - start_forward\n",
    "\n",
    "    if first_token_torch == 0:\n",
    "        first_token_torch = time_forward_torch\n",
    "    # forward the model to get the logits for the index in the sequence\n",
    "\n",
    "    idx_next = get_next_token(logits, temperature, top_k)\n",
    "    \n",
    "    # append sampled index to the running sequence and continue\n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "\n",
    "tot_generation_torch = time.time() - start_generation\n",
    "\n",
    "print(enc.decode(x[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot generation time: 137.9171919822693\n",
      "Tot forward time: 137.38418817520142\n",
      "Time To First Token: 0.3134133815765381\n",
      "Tokens per Second: 0.7250727669459516\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tot generation time: {tot_generation_torch}\")\n",
    "print(f\"Tot forward time: {time_forward_torch}\")\n",
    "print(f\"Time To First Token: {first_token_torch}\")\n",
    "print(f\"Tokens per Second: {token_to_generate / tot_generation_torch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
