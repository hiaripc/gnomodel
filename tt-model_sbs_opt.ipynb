{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:20:51.398 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-11 16:20:51.467 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-11 16:20:51.468 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-11 16:20:51.469 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 16:20:51.470 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-11 16:20:51.470 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-11 16:20:51.471 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-11 16:20:51.471 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-11 16:20:51.472 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-11 16:20:51.473 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-11 16:20:51.473 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-11 16:20:51.474 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-11 16:20:51.475 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-11 16:20:51.475 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-11 16:20:51.476 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-11 16:20:51.476 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-11 16:20:51.482 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 16:20:51.483 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "from typing import Tuple\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2024-12-11 16:20:51.525\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-11 16:20:51.525\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-11 16:20:51.528\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 1 PCI device : [0]\n",
      "\u001b[32m2024-12-11 16:20:51.529\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 64, 'n_layers': 5, 'n_heads': 8, 'n_kv_heads': 4, 'vocab_size': 512, 'multiple_of': 4, 'max_seq_len': 512, 'dropout': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint dict\n",
    "import os\n",
    "checkpoint_dict = torch.load(\"llama2/configs/stories260K.pth\")\n",
    "model_args = checkpoint_dict['model_args']\n",
    "print(model_args)\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "args = ModelArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokens_num = args.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2500, 0.5000, 0.7500])\n",
      "10000.0\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import precompute_freqs_cis\n",
    "torch_freqs_cos, torch_freqs_sin = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "# freqs_cos = ttnn.from_torch(torch_freqs_cos, device=device)\n",
    "#freqs_sin = ttnn.from_torch(torch_freqs_sin, device=device)\n",
    "freqs_cos = torch_freqs_cos\n",
    "freqs_sin = torch_freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: ttnn.Tensor, x: ttnn.Tensor):\n",
    "    ndim = len(x.shape)\n",
    "    assert 0 <= 1 < ndim\n",
    "    print(f\"{freqs_cis.shape}, {x.shape[1]},{x.shape[-1]}\")\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]) \n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    print(freqs_cis.shape)\n",
    "    print(shape)\n",
    "    return ttnn.reshape(freqs_cis, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: ttnn.Tensor,\n",
    "    xk: ttnn.Tensor,\n",
    "    freqs_cos: ttnn.Tensor,\n",
    "    freqs_sin: ttnn.Tensor\n",
    ") -> Tuple[ttnn.Tensor, ttnn.Tensor]:\n",
    "    print(\"xq:\", xq.shape)\n",
    "    assert tuple(xq.shape)[0] == 1, \"Only works with batch 1 :-C\"\n",
    "    xq = ttnn.reshape(xq, (tuple(xq.shape)[:-1] + (-1,2)))\n",
    "    # Cannot unbind, cannot slice with [:..], must use ttnn.slice\n",
    "    # xq_r, xq_i = xq.unbind(-1)\n",
    "    # Squeeze because to_layour only supports 4D max tensor\n",
    "    xq = ttnn.squeeze(xq, 0)\n",
    "    xq = ttnn.to_layout(xq, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xq = ttnn.unsqueeze(xq, 0)\n",
    "    xq_r = ttnn.slice(xq, [0,0,0,0,0], list(tuple(xq.shape)[:-1] + (1,)))\n",
    "    xq_r = ttnn.squeeze(xq_r, -1)\n",
    "    print(\"xq_r:\", xq_r.shape)\n",
    "    # ttnn.deallocate(xq_r)\n",
    "    xq_i = ttnn.slice(xq, [0,0,0,0,1], list(tuple(xq.shape)[:-1] + (2,)))\n",
    "    xq_i = ttnn.squeeze(xq_i, -1)    \n",
    "    print(\"xq_i:\", xq_i.shape)\n",
    "\n",
    "    xk = ttnn.squeeze(xk, 0)\n",
    "    xk = ttnn.to_layout(xk, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xk = ttnn.unsqueeze(xk, 0)\n",
    "    xk = ttnn.unsqueeze(xk, 0)\n",
    "    xk_r = ttnn.slice(xk, [0,0,0,0,0],tuple(xk.shape)[:-1] + (1,))\n",
    "    xk_r = ttnn.squeeze(xk_r, -1)\n",
    "    # ttnn.deallocate(xk_r)\n",
    "    xk_i = ttnn.slice(xk, [0,0,0,0,1], tuple(xk.shape)[:-1] + (2,))\n",
    "    xk_i = ttnn.squeeze(xk_i, -1)  \n",
    "\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    freqs_sin = ttnn.to_layout(freqs_sin, layout = ttnn.TILE_LAYOUT)\n",
    "    freqs_cos = ttnn.to_layout(freqs_cos, layout = ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # apply rotation using real numbers\n",
    "    xq_r = ttnn.to_layout(xq_r, layout = ttnn.TILE_LAYOUT)\n",
    "    xq_i = ttnn.to_layout(xq_i, layout = ttnn.TILE_LAYOUT)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "\n",
    "    xk_r = ttnn.to_layout(xk_r, layout = ttnn.TILE_LAYOUT)\n",
    "    xk_i = ttnn.to_layout(xk_i, layout = ttnn.TILE_LAYOUT)\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    \n",
    "    # there's no ttnn.stack nor ttnn.flatten :-)\n",
    "    xq_out_r = ttnn.to_layout(xq_out_r, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xq_out_i = ttnn.to_layout(xq_out_i, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    print(xq_out_r.shape, xq_out_i.shape)\n",
    "    # Create new dimension\n",
    "    xq_out_r = ttnn.unsqueeze(xq_out_r, -1)\n",
    "    # Concatenate along the new dimension\n",
    "    xq_out = torch.concatenate([xq_out_r, xq_out_i], dim=-1)\n",
    "    # todo: implement flatten\n",
    "    print(xq_out.shape)\n",
    "\n",
    "    # xq_out = ttnn.flatten(xq_out, 3)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.model import apply_rotary_emb as apply_rotary_emb_torch\n",
    "\n",
    "def apply_rotary_emb_host(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq = ttnn.to_torch(xq)\n",
    "    xk = ttnn.to_torch(xk) \n",
    "    xq_out, xk_out = apply_rotary_emb_torch(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "    return ttnn.from_torch(xq_out, device=device), ttnn.from_torch(xk_out, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: ttnn.Tensor, n_rep: int) -> ttnn.Tensor:\n",
    "    return ttnn.repeat_interleave(x, dim=2, repeats=n_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttnn.model_preprocessing import (\n",
    "    preprocess_linear_bias,\n",
    "    preprocess_linear_weight,\n",
    ")\n",
    "\n",
    "class Attention(LightweightModule):\n",
    "    def __init__(self, args: ModelArgs, state_dict: dict, layer_num, device):\n",
    "        super().__init__()\n",
    "        self.state_dict = state_dict\n",
    "        self.max_batch_size = 1\n",
    "        self.device = device\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_q_heads = args.n_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads        \n",
    "        # Indicates how many times the Keys and Values should be repeated        \n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        \n",
    "        prefix = f\"layers.{layer_num}.attention.\"\n",
    "\n",
    "        wq = torch.transpose(self.state_dict[f\"{prefix}wq.weight\"], -2, -1,)\n",
    "        wk = torch.transpose(self.state_dict[f\"{prefix}wk.weight\"], -2, -1,)\n",
    "        wv = torch.transpose(self.state_dict[f\"{prefix}wv.weight\"], -2, -1,)\n",
    "\n",
    "        torch_wqkv = torch.cat([wq, wk, wv], dim=-1)\n",
    "        self.wqkv = preprocess_linear_weight(torch_wqkv.T, dtype=ttnn.bfloat16)\n",
    "        self.wqkv = ttnn.to_device(self.wqkv, device=self.device)\n",
    "\n",
    "        print(self.wqkv.shape)\n",
    "\n",
    "        self.wo = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wo.weight\"], -2, -1,),\n",
    "            layout=ttnn.TILE_LAYOUT,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            device=self.device,\n",
    "            dtype=ttnn.bfloat8_b\n",
    "        )\n",
    "\n",
    "    def forward(self, x: ttnn.Tensor, freqs_cos:torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        fused_qkv_output = ttnn.linear(\n",
    "            x,\n",
    "            self.wqkv,\n",
    "            bias=None,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "            # core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),\n",
    "        )\n",
    "        print(fused_qkv_output.shape)\n",
    "        \n",
    "        assert self.n_kv_heads == self.n_q_heads\n",
    "\n",
    "        # Always | FATAL    | \n",
    "        # Invalid head size: 16. The head size must be a multiple of the tile width (32). Please adjust the dimensions accordingly.\n",
    "        # 32??\n",
    "        xq, xk, xv = ttnn.transformer.split_query_key_value_and_split_heads(\n",
    "            fused_qkv_output,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            num_heads=self.n_kv_heads,\n",
    "        )\n",
    "\n",
    "        # Apply RoPE\n",
    "        xq, xk = apply_rotary_emb_host(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        print(xk.shape)\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        print(xk.shape)\n",
    "\n",
    "        # premute instead of transpose \n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = ttnn.permute(xq, (0, 2, 1, 3))\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        xk = ttnn.permute(xk, (0, 2, 1, 3))\n",
    "        xv = ttnn.permute(xv, (0, 2, 1, 3))\n",
    "\n",
    "        xq = ttnn.to_layout(xq, layout=ttnn.TILE_LAYOUT)\n",
    "        xk = ttnn.to_layout(xk, layout=ttnn.TILE_LAYOUT)\n",
    "        xv = ttnn.to_layout(xv, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "        print(xq.get_dtype(), xq.shape)\n",
    "        print(xk.get_dtype(), xk.shape)\n",
    "        print(xv.get_dtype(), xv.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        output = ttnn.transformer.scaled_dot_product_attention(\n",
    "            xq, \n",
    "            xk, \n",
    "            xv, \n",
    "            attn_mask=None, \n",
    "            is_causal=True\n",
    "        )\n",
    "        \"\"\"\n",
    "        attention_scores = ttnn.matmul(\n",
    "            xq,\n",
    "            xk,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat16,\n",
    "            # core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),\n",
    "        )\n",
    "        ttnn.deallocate(xq)\n",
    "        ttnn.deallocate(xk)\n",
    "\n",
    "        return xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttnn.Shape([64, 128])\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "ttnn.Shape([1, 512, 128])\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;000;000mFATAL   \u001b[0m | Invalid head size: 16. The head size must be a multiple of the tile width (32). Please adjust the dimensions accordingly.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TT_FATAL @ ../ttnn/cpp/ttnn/operations/transformer/split_query_key_value_and_split_heads/split_query_key_value_and_split_heads.cpp:156: head_size % tt::constants::TILE_WIDTH == 0\ninfo:\nInvalid head size: 16. The head size must be a multiple of the tile width (32). Please adjust the dimensions accordingly.\nbacktrace:\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x691748) [0x7f3f999f4748]\n --- ttnn::operations::transformer::SplitQueryKeyValueAndSplitHeadsOperation::invoke(tt::tt_metal::Tensor const&, std::__1::optional<tt::tt_metal::Tensor> const&, unsigned int, std::__1::optional<unsigned int>, bool, std::__1::optional<tt::tt_metal::MemoryConfig> const&)\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2057079) [0x7f3f9b3ba079]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20587d3) [0x7f3f9b3bb7d3]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2058e79) [0x7f3f9b3bbe79]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20567b5) [0x7f3f9b3b97b5]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2055277) [0x7f3f9b3b8277]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2054ae6) [0x7f3f9b3b7ae6]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20538b0) [0x7f3f9b3b68b0]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x166d1e6) [0x7f3f9a9d01e6]\n --- /home/bach/tt-menv/bin/python(PyCFunction_Call+0x59) [0x5f53d9]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python() [0x50b0b8]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python() [0x59c1cc]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x27e) [0x5f4d6e]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1f35) [0x56c645]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python() [0x59bebf]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x630c) [0x570a1c]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(PyEval_EvalCode+0x27) [0x68d2b7]\n --- /home/bach/tt-menv/bin/python() [0x6001d4]\n --- /home/bach/tt-menv/bin/python() [0x5c3a90]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python() [0x503f46]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python() [0x50a98c]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1f35) [0x56c645]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python() [0x50aa00]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1882) [0x56bf92]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /usr/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so(+0x7ef9) [0x7f3ff2ca3ef9]\n --- /usr/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so(+0x9083) [0x7f3ff2ca5083]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python() [0x5fed13]\n --- /home/bach/tt-menv/bin/python() [0x5c3977]\n --- /home/bach/tt-menv/bin/python(PyVectorcall_Call+0x18d) [0x5f525d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x6a6a) [0x57117a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python() [0x50aa00]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x5809) [0x56ff19]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(PyEval_EvalCode+0x27) [0x68d2b7]\n --- /home/bach/tt-menv/bin/python() [0x6001d4]\n --- /home/bach/tt-menv/bin/python() [0x5c3a90]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python() [0x6b6a92]\n --- /home/bach/tt-menv/bin/python(Py_RunMain+0x379) [0x6b6e99]\n --- /home/bach/tt-menv/bin/python(Py_BytesMain+0x2d) [0x6b70bd]\n --- /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f3ff3554083]\n --- /home/bach/tt-menv/bin/python(_start+0x2e) [0x5fa3ae]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m x_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((batch_size,tokens_num,args\u001b[38;5;241m.\u001b[39mdim)) \n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mfrom_torch(\n\u001b[1;32m      8\u001b[0m     x_torch,\n\u001b[1;32m      9\u001b[0m     layout\u001b[38;5;241m=\u001b[39mttnn\u001b[38;5;241m.\u001b[39mTILE_LAYOUT, \n\u001b[1;32m     10\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m xq \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, freqs_cos, freqs_sin)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(fused_qkv_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_q_heads\n\u001b[0;32m---> 55\u001b[0m xq, xk, xv \u001b[38;5;241m=\u001b[39m \u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_query_key_value_and_split_heads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused_qkv_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL1_MEMORY_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Apply RoPE\u001b[39;00m\n\u001b[1;32m     62\u001b[0m xq, xk \u001b[38;5;241m=\u001b[39m apply_rotary_emb_host(xq, xk, freqs_cos, freqs_sin)\n",
      "File \u001b[0;32m~/tt-install/tt-metal/ttnn/ttnn/decorators.py:329\u001b[0m, in \u001b[0;36mFastOperation.__call__\u001b[0;34m(self, *function_args, **function_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfunction_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TT_FATAL @ ../ttnn/cpp/ttnn/operations/transformer/split_query_key_value_and_split_heads/split_query_key_value_and_split_heads.cpp:156: head_size % tt::constants::TILE_WIDTH == 0\ninfo:\nInvalid head size: 16. The head size must be a multiple of the tile width (32). Please adjust the dimensions accordingly.\nbacktrace:\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x691748) [0x7f3f999f4748]\n --- ttnn::operations::transformer::SplitQueryKeyValueAndSplitHeadsOperation::invoke(tt::tt_metal::Tensor const&, std::__1::optional<tt::tt_metal::Tensor> const&, unsigned int, std::__1::optional<unsigned int>, bool, std::__1::optional<tt::tt_metal::MemoryConfig> const&)\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2057079) [0x7f3f9b3ba079]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20587d3) [0x7f3f9b3bb7d3]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2058e79) [0x7f3f9b3bbe79]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20567b5) [0x7f3f9b3b97b5]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2055277) [0x7f3f9b3b8277]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x2054ae6) [0x7f3f9b3b7ae6]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x20538b0) [0x7f3f9b3b68b0]\n --- /home/bach/tt-install/tt-metal/ttnn/ttnn/_ttnn.so(+0x166d1e6) [0x7f3f9a9d01e6]\n --- /home/bach/tt-menv/bin/python(PyCFunction_Call+0x59) [0x5f53d9]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python() [0x50b0b8]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python() [0x59c1cc]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x27e) [0x5f4d6e]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1f35) [0x56c645]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python() [0x59bebf]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x630c) [0x570a1c]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(PyEval_EvalCode+0x27) [0x68d2b7]\n --- /home/bach/tt-menv/bin/python() [0x6001d4]\n --- /home/bach/tt-menv/bin/python() [0x5c3a90]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python() [0x503f46]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python() [0x50a98c]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1f35) [0x56c645]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python() [0x50aa00]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x1882) [0x56bf92]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x213a) [0x56c84a]\n --- /home/bach/tt-menv/bin/python() [0x500328]\n --- /usr/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so(+0x7ef9) [0x7f3ff2ca3ef9]\n --- /usr/lib/python3.8/lib-dynload/_asyncio.cpython-38-x86_64-linux-gnu.so(+0x9083) [0x7f3ff2ca5083]\n --- /home/bach/tt-menv/bin/python(_PyObject_MakeTpCall+0x29e) [0x5f5fae]\n --- /home/bach/tt-menv/bin/python() [0x5fed13]\n --- /home/bach/tt-menv/bin/python() [0x5c3977]\n --- /home/bach/tt-menv/bin/python(PyVectorcall_Call+0x18d) [0x5f525d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x6a6a) [0x57117a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x1b6) [0x5f5786]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x858) [0x56af68]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python() [0x50aa00]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x5809) [0x56ff19]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(PyEval_EvalCode+0x27) [0x68d2b7]\n --- /home/bach/tt-menv/bin/python() [0x6001d4]\n --- /home/bach/tt-menv/bin/python() [0x5c3a90]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalFrameDefault+0x72d) [0x56ae3d]\n --- /home/bach/tt-menv/bin/python(_PyEval_EvalCodeWithName+0x26a) [0x56910a]\n --- /home/bach/tt-menv/bin/python(_PyFunction_Vectorcall+0x393) [0x5f5963]\n --- /home/bach/tt-menv/bin/python(PyObject_Call+0x1f7) [0x5f4ce7]\n --- /home/bach/tt-menv/bin/python() [0x6b6a92]\n --- /home/bach/tt-menv/bin/python(Py_RunMain+0x379) [0x6b6e99]\n --- /home/bach/tt-menv/bin/python(Py_BytesMain+0x2d) [0x6b70bd]\n --- /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f3ff3554083]\n --- /home/bach/tt-menv/bin/python(_start+0x2e) [0x5fa3ae]\n"
     ]
    }
   ],
   "source": [
    "layer_num = 0\n",
    "attention = Attention(args, state_dict, layer_num, device)\n",
    "# attention(ttnn.from_torch(torch.rand((1,64)), layout=ttnn.TILE_LAYOUT), 1)\n",
    "\n",
    "x_torch = torch.rand((batch_size,tokens_num,args.dim)) \n",
    "\n",
    "x = ttnn.from_torch(\n",
    "    x_torch,\n",
    "    layout=ttnn.TILE_LAYOUT, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "xq = attention.forward(x, freqs_cos, freqs_sin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
