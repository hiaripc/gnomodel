{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:33:06.624 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-09 16:33:06.683 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-09 16:33:06.684 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-09 16:33:06.685 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-09 16:33:06.685 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-09 16:33:06.686 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-09 16:33:06.686 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-09 16:33:06.687 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-09 16:33:06.688 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-09 16:33:06.688 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-09 16:33:06.689 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-09 16:33:06.690 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-09 16:33:06.690 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-09 16:33:06.691 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-09 16:33:06.692 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-09 16:33:06.693 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-09 16:33:06.698 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-09 16:33:06.699 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from model import ModelArgs\n",
    "from typing import Tuple\n",
    "import json\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the TinyStories 260k model\n",
    "    dim: int = 64\n",
    "    n_layers: int = 5\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: Optional[int] = 4\n",
    "    vocab_size: int = 512\n",
    "    hidden_dim: Optional[int] = None\n",
    "    multiple_of: int = 4  # MLP hidden layer size will be multiple of\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 512\n",
    "    dropout: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_rotatory_emb(self, xqkv_fused, rot_mat, current_pos, page_table=None):\n",
    "    ##\n",
    "    # Reshape and rotary embeddings\n",
    "    ###\n",
    "    (\n",
    "        q_heads_pre_rot_1BQD,\n",
    "        k_heads_pre_rot_1BKD,\n",
    "        v_heads_1BKD,\n",
    "    ) = ttnn.experimental.nlp_create_qkv_heads_decode(\n",
    "        xqkv_fused,\n",
    "        num_heads=self.n_local_heads,\n",
    "        num_kv_heads=self.n_local_kv_heads,\n",
    "        memory_config=ttnn.L1_HEIGHT_SHARDED_MEMORY_CONFIG,\n",
    "    )\n",
    "\n",
    "    ttnn.deallocate(xqkv_fused)\n",
    "\n",
    "    q_heads_1BQD = ttnn.linear(\n",
    "        q_heads_pre_rot_1BQD,\n",
    "        rot_mat,\n",
    "        program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "            q_heads_pre_rot_1BQD.shape[-2], q_heads_pre_rot_1BQD.shape[-1], rot_mat.shape[-1]\n",
    "        ),\n",
    "        memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "        compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "        dtype=ttnn.bfloat16,\n",
    "    )\n",
    "\n",
    "    k_heads_1BKD = ttnn.linear(\n",
    "        k_heads_pre_rot_1BKD,\n",
    "        rot_mat,\n",
    "        program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "            k_heads_pre_rot_1BKD.shape[-2], k_heads_pre_rot_1BKD.shape[-1], rot_mat.shape[-1]\n",
    "        ),\n",
    "        memory_config=k_heads_pre_rot_1BKD.memory_config(),\n",
    "        compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "        dtype=ttnn.bfloat16,\n",
    "    )\n",
    "\n",
    "    ttnn.deallocate(q_heads_pre_rot_1BQD)\n",
    "    ttnn.deallocate(k_heads_pre_rot_1BKD)\n",
    "\n",
    "    ###\n",
    "    # KV update\n",
    "    ###\n",
    "    keys = self.layer_past[0]\n",
    "    values = self.layer_past[1]\n",
    "\n",
    "    # k_heads, [seqlen, n_kv_heads, bsz, head_dim]\n",
    "    # v_heads [seqlen, n_kv_heads, bsz, head_dim]\n",
    "    # keys, [max_batch_size, n_kv_heads // configuration.num_devices, sliding_window, head_dim]\n",
    "    ttnn.experimental.paged_update_cache(\n",
    "        keys, k_heads_1BKD, update_idxs_tensor=current_pos, page_table=page_table)\n",
    "    \n",
    "    ttnn.experimental.paged_update_cache(\n",
    "        values, v_heads_1BKD, update_idxs_tensor=current_pos, page_table=page_table)\n",
    "    \n",
    "    self.layer_past[0] = keys\n",
    "    self.layer_past[1] = values\n",
    "\n",
    "    ttnn.deallocate(k_heads_1BKD)\n",
    "    ttnn.deallocate(v_heads_1BKD)\n",
    "\n",
    "    if page_table:\n",
    "        attn_output_1G4D = ttnn.transformer.paged_scaled_dot_product_attention_decode(\n",
    "            q_heads_1BQD,\n",
    "            keys,\n",
    "            values,\n",
    "            cur_pos_tensor=current_pos,\n",
    "            page_table_tensor=page_table,\n",
    "            scale=self.scale,\n",
    "            program_config=self.model_config[\"SDPA_DECODE_PROGCFG\"],\n",
    "            compute_kernel_config=self.model_config[\"SDPA_DECODE_COMPUTE_PROGCFG\"],\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "        )\n",
    "    else:\n",
    "        attn_output_1G4D = ttnn.transformer.scaled_dot_product_attention_decode(\n",
    "            q_heads_1BQD,\n",
    "            keys,\n",
    "            values,\n",
    "            cur_pos_tensor=current_pos,\n",
    "            scale=self.scale,\n",
    "            program_config=self.model_config[\"SDPA_DECODE_PROGCFG\"],\n",
    "            compute_kernel_config=self.model_config[\"SDPA_DECODE_COMPUTE_PROGCFG\"],\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG,  # FIXME: why not L1 height sharded e.g. SCORES_BATCHED_MM_OUTPUT_MEMCFG?\n",
    "        )\n",
    "\n",
    "    ttnn.deallocate(q_heads_1BQD)\n",
    "\n",
    "    attn_output_11BH = ttnn.to_memory_config(\n",
    "        attn_output_1G4D, memory_config=self.model_config[\"SCORES_BATCHED_MM_OUTPUT_MEMCFG\"]\n",
    "    )\n",
    "    attn_output_cat = ttnn.experimental.nlp_concat_heads_decode(\n",
    "        attn_output_11BH,\n",
    "        num_heads=self.n_local_heads,\n",
    "    )\n",
    "    ttnn.deallocate(attn_output_11BH)\n",
    "    ttnn.deallocate(attn_output_1G4D)\n",
    "\n",
    "    return attn_output_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated (3674697399.py, line 119)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 119\u001b[0;36m\u001b[0m\n\u001b[0;31m    memory_config=ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated\n"
     ]
    }
   ],
   "source": [
    "class Attention(LightweightModule):\n",
    "    def __init__(self, args: ModelArgs, state_dict: dict, layer_num, device):\n",
    "        super().__init__()\n",
    "        self.state_dict = state_dict\n",
    "        self.max_batch_size = 1\n",
    "        self.device = device\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_q_heads = args.n_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads        \n",
    "        # Indicates how many times the Keys and Values should be repeated        \n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        \n",
    "        prefix = f\"layers.{layer_num}.attention.\"\n",
    "\n",
    "        ## Concatenated weights\n",
    "        self.wqkv = ttnn.as_tensor(\n",
    "                torch.concat([\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wq.weight\"], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wk.weight\"], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wv.weight\"], -2, -1,),\n",
    "                    ],\n",
    "                    dim=-1\n",
    "                ),\n",
    "                # device=self.device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-1),\n",
    "                # dtype=self.dtype,\n",
    "                # memory_config=wqkv_mem_config,\n",
    "                # layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                # cache_file_name=cache_name(\"wqkv_sharded\"),\n",
    "        )\n",
    "        \n",
    "        ## Output weights\n",
    "        self.wo = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wo.weight\"], -2, -1,),\n",
    "            # device=self.device,\n",
    "            # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-2),\n",
    "            # dtype=self.dtype,\n",
    "            # memory_config=wo_mem_config,\n",
    "            # layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "            # cache_file_name=cache_name(\"wo_height_sharded\"),\n",
    "            )\n",
    "\n",
    "        ## Cache KV -> can be introduced later\n",
    "        \"\"\"\n",
    "        cache_k = torch.zeros((\n",
    "                    self.max_batch_size,\n",
    "                    self.n_kv_heads,\n",
    "                    self.sliding_window,\n",
    "                    self.head_dim,)\n",
    "        )\n",
    "        \n",
    "        cache_v = torch.zeros((\n",
    "                self.max_batch_size,\n",
    "                self.n_kv_heads,\n",
    "                self.sliding_window,\n",
    "                self.head_dim,)\n",
    "        )\n",
    "\n",
    "        self.layer_past = [\n",
    "            ttnn.as_tensor(\n",
    "                k_or_v,\n",
    "                device=self.mesh_device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=1),\n",
    "                layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                dtype=self.dtype,\n",
    "                # cache_file_name=f\"{weight_cache_path}/kvcache_{k_or_v.shape}\"\n",
    "                # if weight_cache_path and not configuration.dummy_weights\n",
    "                # else None,\n",
    "                memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "            )\n",
    "            for k_or_v in [cache_k, cache_v]\n",
    "        ]\n",
    "\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"YEAH\")\n",
    "\n",
    "    \n",
    "    def forward(self, x, current_pos, rot_mats=None, transformation_mats=None, user_id=0, mode=\"decode\", page_table=None\n",
    "    ):\n",
    "        # Two inference phase, prefil and decode\n",
    "        # Prefil: generate KV cache for subsequent tokens generations\n",
    "        # Decode: generate tokens\n",
    "        if mode == \"prefill\":\n",
    "            return self.forward_prefill(x, rot_mats, transformation_mats, user_id, page_table)\n",
    "        else:\n",
    "            return self.forward_decode(x, current_pos, rot_mats, page_table)\n",
    "\n",
    "    def forward_prefill(self, x_11SH, rot_mats, transformation_mats, user_id: int = 0, page_table=None):\n",
    "        # todo\n",
    "        ...\n",
    "\n",
    "    def forward_decode(self, x: ttnn.Tensor, current_pos, rot_mat=None, page_table=None,) -> ttnn.Tensor:\n",
    "        \"\"\"\n",
    "        x: (seq_len, 1, batch, dim)\n",
    "        current_pos: (batch_size), current token position in the sequence for each user\n",
    "        \"\"\"\n",
    "        assert self.max_batch_size * self.n_kv_heads < 64\n",
    "        ###\n",
    "        # QKV matmuls\n",
    "        # Use HiFi2 for DRAM-sharded matmuls as they are otherwise flop-bound. Loses 1 bit of activation precision.\n",
    "        ###\n",
    "        \n",
    "        self.compute_kernel_config_hifi2 = ttnn.GrayskullComputeKernelConfig(\n",
    "                math_fidelity=ttnn.MathFidelity.HiFi2,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ## Linear transformation with qkv concatened\n",
    "        xqkv_fused_sharded = ttnn.linear(\n",
    "            x,\n",
    "            self.wqkv,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            memory_config=ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,\n",
    "            # program_config=self.model_config[\"XQKV_DECODE_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            device=self.device,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "        ttnn.deallocate(x)\n",
    "\n",
    "        ## ??? I suppose to join sharded qkv but sharded where? devices or cores?\n",
    "        xqkv_fused = ttnn.sharded_to_interleaved(xqkv_fused_sharded, ttnn.L1_MEMORY_CONFIG)\n",
    "        ttnn.deallocate(xqkv_fused_sharded)\n",
    "\n",
    "        # Reshape such that true unpadded batch is tracked in shape\n",
    "        fqkv_shape = xqkv_fused.shape\n",
    "        xqkv_fused = ttnn.reshape(\n",
    "            xqkv_fused, ttnn.Shape((1, 1, self.max_batch_size, fqkv_shape[3]), (1, 1, 32, fqkv_shape[3]))\n",
    "        )\n",
    "\n",
    "        # RoPE relative positional embeddings\n",
    "        attn_output_cat = self.apply_rotatory_emb(xqkv_fused)        \n",
    "\n",
    "\n",
    "        # program config matched to output of nlp_concat_heads_decode\n",
    "        dense_out_sharded = ttnn.linear(\n",
    "            attn_output_cat,\n",
    "            self.wo,\n",
    "            # program_config=self.model_config[\"ATTN_OUTPUT_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            memory_config=attn_output_cat.memory_config(),\n",
    "        )  # seqlen, 1, batch, hidden_size\n",
    "\n",
    "        ttnn.deallocate(attn_output_cat)\n",
    "\n",
    "        dense_out_sharded = ttnn.to_memory_config(dense_out_sharded, self.model_config[\"DECODE_RESIDUAL_MEMCFG\"])\n",
    "        return dense_out_sharded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 64, 'n_layers': 5, 'n_heads': 8, 'n_kv_heads': 4, 'vocab_size': 512, 'multiple_of': 4, 'max_seq_len': 512, 'dropout': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint dict\n",
    "import os\n",
    "checkpoint_dict = torch.load(\"configs/stories260K.pth\")\n",
    "model_args = checkpoint_dict['model_args']\n",
    "print(model_args)\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "YEAH\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.matmul.linear_t, input_tensor_a: ttnn._ttnn.tensor.Tensor, input_tensor_b: ttnn._ttnn.tensor.Tensor, *, bias: Optional[ttnn._ttnn.tensor.Tensor] = None, transpose_a: bool = False, transpose_b: bool = False, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, program_config: Optional[Union[ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig]] = None, activation: Optional[str] = None, compute_kernel_config: Optional[Union[ttnn._ttnn.operations.core.GrayskullComputeKernelConfig, ttnn._ttnn.operations.core.WormholeComputeKernelConfig]] = None, core_grid: Optional[ttnn._ttnn.types.CoreGrid] = None, output_tile: Optional[ttnn._ttnn.tensor.Tile] = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.matmul.linear_t object at 0x7fda2fcbc270>, ttnn.Tensor([[ 0.31300,  0.40563,  ...,  0.00000,  0.00000],\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000],\n             ...,\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000],\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000]], shape=Shape([1[32], 64]), dtype=DataType::FLOAT32, layout=Layout::TILE), ttnn.Tensor([[ 0.04439,  0.22455,  ..., -0.07298, -0.04899],\n             [ 0.07975,  0.12867,  ...,  0.01960, -0.02137],\n             ...,\n             [-0.01281, -0.04828,  ...,  0.01264,  0.01113],\n             [-0.09789, -0.18994,  ..., -0.00464,  0.05898]], shape=Shape([64, 128]), dtype=DataType::FLOAT32, layout=Layout::ROW_MAJOR); kwargs: memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::INTERLEAVED,buffer_type=BufferType::L1,shard_spec=std::nullopt), device=<ttnn._ttnn.device.Device object at 0x7fda2f636530>, dtype=<DataType.BFLOAT16: 0>\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m layer_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m attention \u001b[38;5;241m=\u001b[39m Attention(args, state_dict, layer_num, device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTILE_LAYOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tt/tt-nn/gnomodel/ttcode/lightweightmodule.py:12\u001b[0m, in \u001b[0;36mLightweightModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 92\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, current_pos, rot_mats, transformation_mats, user_id, mode, page_table)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_prefill(x, rot_mats, transformation_mats, user_id, page_table)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrot_mats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 115\u001b[0m, in \u001b[0;36mAttention.forward_decode\u001b[0;34m(self, x, current_pos, rot_mat, page_table)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_kernel_config_hifi2 \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mGrayskullComputeKernelConfig(\n\u001b[1;32m    110\u001b[0m         math_fidelity\u001b[38;5;241m=\u001b[39mttnn\u001b[38;5;241m.\u001b[39mMathFidelity\u001b[38;5;241m.\u001b[39mHiFi2,\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m## Linear transformation with qkv concatened\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m xqkv_fused_sharded \u001b[38;5;241m=\u001b[39m \u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL1_MEMORY_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# memory_config=ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,\u001b[39;49;00m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# program_config=self.model_config[\"XQKV_DECODE_PROGCFG\"],\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# compute_kernel_config=self.compute_kernel_config_hifi2,\u001b[39;49;00m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mttnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m ttnn\u001b[38;5;241m.\u001b[39mdeallocate(x)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m## ??? I suppose to join sharded qkv but sharded where? devices or cores?\u001b[39;00m\n",
      "File \u001b[0;32m~/tt-install/tt-metal/ttnn/ttnn/decorators.py:329\u001b[0m, in \u001b[0;36mFastOperation.__call__\u001b[0;34m(self, *function_args, **function_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mfunction_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_kwargs):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: ttnn._ttnn.operations.matmul.linear_t, input_tensor_a: ttnn._ttnn.tensor.Tensor, input_tensor_b: ttnn._ttnn.tensor.Tensor, *, bias: Optional[ttnn._ttnn.tensor.Tensor] = None, transpose_a: bool = False, transpose_b: bool = False, memory_config: Optional[ttnn._ttnn.tensor.MemoryConfig] = None, dtype: Optional[ttnn._ttnn.tensor.DataType] = None, program_config: Optional[Union[ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn._ttnn.operations.matmul.MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig]] = None, activation: Optional[str] = None, compute_kernel_config: Optional[Union[ttnn._ttnn.operations.core.GrayskullComputeKernelConfig, ttnn._ttnn.operations.core.WormholeComputeKernelConfig]] = None, core_grid: Optional[ttnn._ttnn.types.CoreGrid] = None, output_tile: Optional[ttnn._ttnn.tensor.Tile] = None) -> ttnn._ttnn.tensor.Tensor\n\nInvoked with: <ttnn._ttnn.operations.matmul.linear_t object at 0x7fda2fcbc270>, ttnn.Tensor([[ 0.31300,  0.40563,  ...,  0.00000,  0.00000],\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000],\n             ...,\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000],\n             [ 0.00000,  0.00000,  ...,  0.00000,  0.00000]], shape=Shape([1[32], 64]), dtype=DataType::FLOAT32, layout=Layout::TILE), ttnn.Tensor([[ 0.04439,  0.22455,  ..., -0.07298, -0.04899],\n             [ 0.07975,  0.12867,  ...,  0.01960, -0.02137],\n             ...,\n             [-0.01281, -0.04828,  ...,  0.01264,  0.01113],\n             [-0.09789, -0.18994,  ..., -0.00464,  0.05898]], shape=Shape([64, 128]), dtype=DataType::FLOAT32, layout=Layout::ROW_MAJOR); kwargs: memory_config=MemoryConfig(memory_layout=TensorMemoryLayout::INTERLEAVED,buffer_type=BufferType::L1,shard_spec=std::nullopt), device=<ttnn._ttnn.device.Device object at 0x7fda2f636530>, dtype=<DataType.BFLOAT16: 0>\n\nDid you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\n<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\nconversions are optional and require extra headers to be included\nwhen compiling your pybind11 module."
     ]
    }
   ],
   "source": [
    "device = ttnn.open_device(device_id=0)\n",
    "args = ModelArgs()\n",
    "layer_num = 0\n",
    "attention = Attention(args, state_dict, layer_num, device)\n",
    "attention(ttnn.from_torch(torch.rand((1,64)), layout=ttnn.TILE_LAYOUT), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
