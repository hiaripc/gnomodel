{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 11:37:45.723 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-11 11:37:45.779 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-11 11:37:45.780 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-11 11:37:45.781 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 11:37:45.782 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-11 11:37:45.782 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-11 11:37:45.783 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-11 11:37:45.783 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-11 11:37:45.784 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-11 11:37:45.784 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-11 11:37:45.785 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-11 11:37:45.786 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-11 11:37:45.786 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-11 11:37:45.787 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-11 11:37:45.788 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-11 11:37:45.789 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-11 11:37:45.794 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 11:37:45.795 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "from typing import Tuple\n",
    "import json\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule\n",
    "checkpoint = \"llama2/configs/stories260K.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotatory_emb(self, xqkv_fused, rot_mat, current_pos, page_table=None):\n",
    "    ##\n",
    "    # Reshape and rotary embeddings\n",
    "    ###\n",
    "    (\n",
    "        q_heads_pre_rot_1BQD,\n",
    "        k_heads_pre_rot_1BKD,\n",
    "        v_heads_1BKD,\n",
    "    ) = ttnn.experimental.nlp_create_qkv_heads_decode(\n",
    "        xqkv_fused,\n",
    "        num_heads=self.n_local_heads,\n",
    "        num_kv_heads=self.n_local_kv_heads,\n",
    "        memory_config=ttnn.L1_HEIGHT_SHARDED_MEMORY_CONFIG,\n",
    "    )\n",
    "\n",
    "    ttnn.deallocate(xqkv_fused)\n",
    "\n",
    "    q_heads_1BQD = ttnn.linear(\n",
    "        q_heads_pre_rot_1BQD,\n",
    "        rot_mat,\n",
    "        program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "            q_heads_pre_rot_1BQD.shape[-2], q_heads_pre_rot_1BQD.shape[-1], rot_mat.shape[-1]\n",
    "        ),\n",
    "        memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "        compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "        dtype=ttnn.bfloat16,\n",
    "    )\n",
    "\n",
    "    k_heads_1BKD = ttnn.linear(\n",
    "        k_heads_pre_rot_1BKD,\n",
    "        rot_mat,\n",
    "        program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "            k_heads_pre_rot_1BKD.shape[-2], k_heads_pre_rot_1BKD.shape[-1], rot_mat.shape[-1]\n",
    "        ),\n",
    "        memory_config=k_heads_pre_rot_1BKD.memory_config(),\n",
    "        compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "        dtype=ttnn.bfloat16,\n",
    "    )\n",
    "\n",
    "    ttnn.deallocate(q_heads_pre_rot_1BQD)\n",
    "    ttnn.deallocate(k_heads_pre_rot_1BKD)\n",
    "\n",
    "    return q_heads_1BQD, k_heads_1BKD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(LightweightModule):\n",
    "\n",
    "    def kv_cache(self, q_heads_1BQD, k_heads_1BKD, v_heads_1BKD, current_pos):\n",
    "        ###\n",
    "        # KV update\n",
    "        ###\n",
    "        keys = self.layer_past[0]\n",
    "        values = self.layer_past[1]\n",
    "\n",
    "        # k_heads, [seqlen, n_kv_heads, bsz, head_dim]\n",
    "        # v_heads [seqlen, n_kv_heads, bsz, head_dim]\n",
    "        # keys, [max_batch_size, n_kv_heads // configuration.num_devices, sliding_window, head_dim]\n",
    "        ttnn.experimental.paged_update_cache(\n",
    "            keys, k_heads_1BKD, update_idxs_tensor=current_pos\n",
    "            )\n",
    "        \n",
    "        ttnn.experimental.paged_update_cache(\n",
    "            values, v_heads_1BKD, update_idxs_tensor=current_pos\n",
    "            )\n",
    "        \n",
    "        self.layer_past[0] = keys\n",
    "        self.layer_past[1] = values\n",
    "\n",
    "        ttnn.deallocate(k_heads_1BKD)\n",
    "        ttnn.deallocate(v_heads_1BKD)\n",
    "\n",
    "        attn_output_1G4D = ttnn.transformer.scaled_dot_product_attention_decode(\n",
    "            q_heads_1BQD,\n",
    "            keys,\n",
    "            values,\n",
    "            cur_pos_tensor=current_pos,\n",
    "            scale=self.scale,\n",
    "            # program_config=self.model_config[\"SDPA_DECODE_PROGCFG\"],\n",
    "            # compute_kernel_config=self.model_config[\"SDPA_DECODE_COMPUTE_PROGCFG\"],\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG,  # FIXME: why not L1 height sharded e.g. SCORES_BATCHED_MM_OUTPUT_MEMCFG?\n",
    "        )\n",
    "\n",
    "        ttnn.deallocate(q_heads_1BQD)\n",
    "\n",
    "        attn_output_11BH = ttnn.to_memory_config(\n",
    "            attn_output_1G4D, \n",
    "            # memory_config=self.model_config[\"SCORES_BATCHED_MM_OUTPUT_MEMCFG\"]\n",
    "        )\n",
    "        attn_output_cat = ttnn.experimental.nlp_concat_heads_decode(\n",
    "            attn_output_11BH,\n",
    "            num_heads=self.n_local_heads,\n",
    "        )\n",
    "        ttnn.deallocate(attn_output_11BH)\n",
    "        ttnn.deallocate(attn_output_1G4D)\n",
    "\n",
    "        return attn_output_cat\n",
    "\n",
    "    def __init__(self, args: ModelArgs, state_dict: dict, layer_num, device):\n",
    "        super().__init__()\n",
    "        self.state_dict = state_dict\n",
    "        self.max_batch_size = 1\n",
    "        self.device = device\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_q_heads = args.n_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads        \n",
    "        # Indicates how many times the Keys and Values should be repeated        \n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        \n",
    "        prefix = f\"layers.{layer_num}.attention.\"\n",
    "\n",
    "        ## Concatenated weights\n",
    "        self.wqkv = ttnn.as_tensor(\n",
    "                torch.concat([\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wq.weight\"], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wk.weight\"], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[f\"{prefix}wv.weight\"], -2, -1,),\n",
    "                    ],\n",
    "                    dim=-1\n",
    "                ),\n",
    "                # device=self.device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-1),\n",
    "                # dtype=self.dtype,\n",
    "                # memory_config=wqkv_mem_config,\n",
    "                # layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                # cache_file_name=cache_name(\"wqkv_sharded\"),\n",
    "        )\n",
    "        \n",
    "        ## Output weights\n",
    "        self.wo = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wo.weight\"], -2, -1,),\n",
    "            # device=self.device,\n",
    "            # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-2),\n",
    "            # dtype=self.dtype,\n",
    "            # memory_config=wo_mem_config,\n",
    "            # layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "            # cache_file_name=cache_name(\"wo_height_sharded\"),\n",
    "            )\n",
    "\n",
    "        ## Cache KV \n",
    "        \n",
    "        cache_k = torch.zeros((\n",
    "            self.paged_attention_config.max_num_blocks,\n",
    "            self.n_kv_heads,\n",
    "            self.paged_attention_config.block_size,\n",
    "            self.head_dim,)\n",
    "        )\n",
    "        cache_v = torch.zeros((\n",
    "            self.paged_attention_config.max_num_blocks,\n",
    "            self.n_kv_heads,\n",
    "            self.paged_attention_config.block_size,\n",
    "            self.head_dim,)\n",
    "        )\n",
    "\n",
    "        self.layer_past = [\n",
    "            ttnn.as_tensor(\n",
    "                k_or_v,\n",
    "                device=self.mesh_device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=1),\n",
    "                # layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                dtype=self.dtype,\n",
    "                # cache_file_name=f\"{weight_cache_path}/kvcache_{k_or_v.shape}\"\n",
    "                # if weight_cache_path and not configuration.dummy_weights\n",
    "                # else None,\n",
    "                memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "            )\n",
    "            for k_or_v in [cache_k, cache_v]\n",
    "        ]\n",
    "\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        \n",
    "    \n",
    "    def forward(self, x, current_pos, rot_mats=None, transformation_mats=None, user_id=0, mode=\"decode\", page_table=None\n",
    "    ):\n",
    "        # Two inference phase, prefil and decode\n",
    "        # Prefil: generate KV cache for subsequent tokens generations\n",
    "        # Decode: generate tokens\n",
    "        if mode == \"prefill\":\n",
    "            return self.forward_prefill(x, rot_mats, transformation_mats, user_id, page_table)\n",
    "        else:\n",
    "            return self.forward_decode(x, current_pos, rot_mats, page_table)\n",
    "\n",
    "    def forward_prefill(self, x_11SH, rot_mats, transformation_mats, user_id: int = 0, page_table=None):\n",
    "        # todo\n",
    "        ...\n",
    "\n",
    "    def forward_decode(self, x: ttnn.Tensor, current_pos, rot_mat=None, page_table=None,) -> ttnn.Tensor:\n",
    "        \"\"\"\n",
    "        x: (seq_len, 1, batch, dim)\n",
    "        current_pos: (batch_size), current token position in the sequence for each user\n",
    "        \"\"\"\n",
    "        assert self.max_batch_size * self.n_kv_heads < 64\n",
    "        ###\n",
    "        # QKV matmuls\n",
    "        # Use HiFi2 for DRAM-sharded matmuls as they are otherwise flop-bound. Loses 1 bit of activation precision.\n",
    "        ###\n",
    "        \n",
    "        self.compute_kernel_config_hifi2 = ttnn.GrayskullComputeKernelConfig(\n",
    "                math_fidelity=ttnn.MathFidelity.HiFi2,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ## Linear transformation with qkv concatened\n",
    "        xqkv_fused_sharded = ttnn.linear(\n",
    "            x,\n",
    "            self.wqkv,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            # memory_config=ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,\n",
    "            # program_config=self.model_config[\"XQKV_DECODE_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            device=self.device,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "        ttnn.deallocate(x)\n",
    "\n",
    "        ## ??? I suppose to join sharded qkv but sharded where? devices or cores?\n",
    "        xqkv_fused = ttnn.sharded_to_interleaved(xqkv_fused_sharded, ttnn.L1_MEMORY_CONFIG)\n",
    "        ttnn.deallocate(xqkv_fused_sharded)\n",
    "\n",
    "        # Reshape such that true unpadded batch is tracked in shape\n",
    "        fqkv_shape = xqkv_fused.shape\n",
    "        xqkv_fused = ttnn.reshape(\n",
    "            xqkv_fused, ttnn.Shape((1, 1, self.max_batch_size, fqkv_shape[3]), (1, 1, 32, fqkv_shape[3]))\n",
    "        )\n",
    "\n",
    "        # RoPE relative positional embeddings\n",
    "        attn_output_cat = self.apply_rotatory_emb(xqkv_fused)        \n",
    "\n",
    "        # program config matched to output of nlp_concat_heads_decode\n",
    "        dense_out_sharded = ttnn.linear(\n",
    "            attn_output_cat,\n",
    "            self.wo,\n",
    "            # program_config=self.model_config[\"ATTN_OUTPUT_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            memory_config=attn_output_cat.memory_config(),\n",
    "        )  # seqlen, 1, batch, hidden_size\n",
    "\n",
    "        ttnn.deallocate(attn_output_cat)\n",
    "\n",
    "        dense_out_sharded = ttnn.to_memory_config(dense_out_sharded, self.model_config[\"DECODE_RESIDUAL_MEMCFG\"])\n",
    "        return dense_out_sharded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 64, 'n_layers': 5, 'n_heads': 8, 'n_kv_heads': 4, 'vocab_size': 512, 'multiple_of': 4, 'max_seq_len': 512, 'dropout': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint dict\n",
    "import os\n",
    "checkpoint_dict = torch.load(checkpoint)\n",
    "model_args = checkpoint_dict['model_args']\n",
    "print(model_args)\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Attention' object has no attribute 'paged_attention_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m ModelArgs()\n\u001b[1;32m      3\u001b[0m layer_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m attention(ttnn\u001b[38;5;241m.\u001b[39mfrom_torch(torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m64\u001b[39m)), layout\u001b[38;5;241m=\u001b[39mttnn\u001b[38;5;241m.\u001b[39mTILE_LAYOUT), \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 101\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, args, state_dict, layer_num, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mas_tensor(\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mwo.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,),\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# device=self.device,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# cache_file_name=cache_name(\"wo_height_sharded\"),\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m## Cache KV \u001b[39;00m\n\u001b[1;32m    100\u001b[0m cache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaged_attention_config\u001b[49m\u001b[38;5;241m.\u001b[39mmax_num_blocks,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads,\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_config\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,)\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m cache_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_config\u001b[38;5;241m.\u001b[39mmax_num_blocks,\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_config\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,)\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_past \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    114\u001b[0m     ttnn\u001b[38;5;241m.\u001b[39mas_tensor(\n\u001b[1;32m    115\u001b[0m         k_or_v,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k_or_v \u001b[38;5;129;01min\u001b[39;00m [cache_k, cache_v]\n\u001b[1;32m    126\u001b[0m ]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Attention' object has no attribute 'paged_attention_config'"
     ]
    }
   ],
   "source": [
    "device = ttnn.open_device(device_id=0)\n",
    "args = ModelArgs()\n",
    "layer_num = 0\n",
    "attention = Attention(args, state_dict, layer_num, device)\n",
    "attention(ttnn.from_torch(torch.rand((1,64)), layout=ttnn.TILE_LAYOUT), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
