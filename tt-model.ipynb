{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 14:36:24.228 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-05 14:36:24.331 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-05 14:36:24.332 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-05 14:36:24.333 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-05 14:36:24.334 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-05 14:36:24.334 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-05 14:36:24.335 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-05 14:36:24.335 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-05 14:36:24.336 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-05 14:36:24.336 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-05 14:36:24.336 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-05 14:36:24.337 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-05 14:36:24.337 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-05 14:36:24.338 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-05 14:36:24.339 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-05 14:36:24.339 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-05 14:36:24.345 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-05 14:36:24.346 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from model import ModelArgs\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the TinyStories 260k model\n",
    "    dim: int = 64\n",
    "    n_layers: int = 5\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: Optional[int] = 4\n",
    "    vocab_size: int = 512\n",
    "    hidden_dim: Optional[int] = None\n",
    "    multiple_of: int = 4  # MLP hidden layer size will be multiple of\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 512\n",
    "    dropout: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Attention(LightweightModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_q_heads = args.n_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads        \n",
    "        # Indicates how many times the Keys and Values should be repeated        \n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        \n",
    "        layer_name = \"\"\n",
    "        wq_str = f\"{layer_name}.wq.weight\"\n",
    "        wk_str = f\"{layer_name}.wk.weight\"\n",
    "        wv_str = f\"{layer_name}.wv.weight\"\n",
    "        wo_str = f\"{layer_name}.wo.weight\"\n",
    "\n",
    "        ## Concatenated weights\n",
    "        self.wqkv = ttnn.as_tensor(\n",
    "                torch.concat([\n",
    "                        torch.transpose(self.state_dict[wq_str], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[wk_str], -2, -1,),\n",
    "                        torch.transpose(self.state_dict[wv_str], -2, -1,),\n",
    "                    ],\n",
    "                    dim=-1\n",
    "                ),\n",
    "                device=self.mesh_device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-1),\n",
    "                dtype=self.dtype,\n",
    "                # memory_config=wqkv_mem_config,\n",
    "                layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                # cache_file_name=cache_name(\"wqkv_sharded\"),\n",
    "        )\n",
    "        \n",
    "        ## Output weights\n",
    "        self.wo = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[wo_str], -2, -1,),\n",
    "            device=self.mesh_device,\n",
    "            # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=-2),\n",
    "            dtype=self.dtype,\n",
    "            # memory_config=wo_mem_config,\n",
    "            layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "            # cache_file_name=cache_name(\"wo_height_sharded\"),\n",
    "            )\n",
    "\n",
    "        ## Cache KV -> can be introduced later\n",
    "        \"\"\"\n",
    "        cache_k = torch.zeros((\n",
    "                    self.max_batch_size,\n",
    "                    self.n_kv_heads,\n",
    "                    self.sliding_window,\n",
    "                    self.head_dim,)\n",
    "        )\n",
    "        \n",
    "        cache_v = torch.zeros((\n",
    "                self.max_batch_size,\n",
    "                self.n_kv_heads,\n",
    "                self.sliding_window,\n",
    "                self.head_dim,)\n",
    "        )\n",
    "\n",
    "        self.layer_past = [\n",
    "            ttnn.as_tensor(\n",
    "                k_or_v,\n",
    "                device=self.mesh_device,\n",
    "                # mesh_mapper=ttnn.ShardTensorToMesh(self.mesh_device, dim=1),\n",
    "                layout=self.model_config[\"ATTN_W_LAYOUT_TILE\"],\n",
    "                dtype=self.dtype,\n",
    "                # cache_file_name=f\"{weight_cache_path}/kvcache_{k_or_v.shape}\"\n",
    "                # if weight_cache_path and not configuration.dummy_weights\n",
    "                # else None,\n",
    "                memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "            )\n",
    "            for k_or_v in [cache_k, cache_v]\n",
    "        ]\n",
    "\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, current_pos, rot_mats=None, transformation_mats=None, user_id=0, mode=\"decode\", page_table=None\n",
    "    ):\n",
    "        # Two inference phase, prefil and decode\n",
    "        # Prefil: generate KV cache for subsequent tokens generations\n",
    "        # Decode: generate tokenes\n",
    "        if mode == \"prefill\":\n",
    "            return self.forward_prefill(x, rot_mats, transformation_mats, user_id, page_table)\n",
    "        else:\n",
    "            return self.forward_decode(x, current_pos, rot_mats, page_table)\n",
    "\n",
    "    def forward_prefill(self, x_11SH, rot_mats, transformation_mats, user_id: int = 0, page_table=None):\n",
    "        # todo\n",
    "        ...\n",
    "\n",
    "    def forward_decode(self, x: ttnn.Tensor, current_pos, rot_mat=None, page_table=None,) -> ttnn.Tensor:\n",
    "        \"\"\"\n",
    "        x: (seq_len, 1, batch, dim)\n",
    "        current_pos: (batch_size), current token position in the sequence for each user\n",
    "        \"\"\"\n",
    "        assert self.max_batch_size * self.n_kv_heads < 64\n",
    "        ###\n",
    "        # QKV matmuls\n",
    "        # Use HiFi2 for DRAM-sharded matmuls as they are otherwise flop-bound. Loses 1 bit of activation precision.\n",
    "        ###\n",
    "\n",
    "        ## Linear transformation with qkv concatened\n",
    "        xqkv_fused_sharded = ttnn.linear(\n",
    "            x,\n",
    "            self.wqkv,\n",
    "            memory_config=ttnn.L1_WIDTH_SHARDED_MEMORY_CONFIG,\n",
    "            program_config=self.model_config[\"XQKV_DECODE_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "        ttnn.deallocate(x)\n",
    "\n",
    "        ## ??? I suppose to join sharded qkv but sharded where? devices or cores?\n",
    "        xqkv_fused = ttnn.sharded_to_interleaved(xqkv_fused_sharded, ttnn.L1_MEMORY_CONFIG)\n",
    "        ttnn.deallocate(xqkv_fused_sharded)\n",
    "\n",
    "        # Reshape such that true unpadded batch is tracked in shape\n",
    "        fqkv_shape = xqkv_fused.shape\n",
    "        xqkv_fused = ttnn.reshape(\n",
    "            xqkv_fused, ttnn.Shape((1, 1, self.max_batch_size, fqkv_shape[3]), (1, 1, 32, fqkv_shape[3]))\n",
    "        )\n",
    "\n",
    "        # RoPE relative positional embeddings\n",
    "        attn_output_cat = self.apply_rotatory_emb(xqkv_fused)        \n",
    "\n",
    "\n",
    "        # program config matched to output of nlp_concat_heads_decode\n",
    "        dense_out_sharded = ttnn.linear(\n",
    "            attn_output_cat,\n",
    "            self.wo,\n",
    "            program_config=self.model_config[\"ATTN_OUTPUT_PROGCFG\"],\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            memory_config=attn_output_cat.memory_config(),\n",
    "        )  # seqlen, 1, batch, hidden_size\n",
    "\n",
    "        ttnn.deallocate(attn_output_cat)\n",
    "\n",
    "        dense_out_sharded = ttnn.to_memory_config(dense_out_sharded, self.model_config[\"DECODE_RESIDUAL_MEMCFG\"])\n",
    "        return dense_out_sharded\n",
    "\n",
    "\n",
    "    def apply_rotatory_emb(self, xqkv_fused, rot_mat, current_pos, page_table=None):\n",
    "        ##\n",
    "        # Reshape and rotary embeddings\n",
    "        ###\n",
    "        (\n",
    "            q_heads_pre_rot_1BQD,\n",
    "            k_heads_pre_rot_1BKD,\n",
    "            v_heads_1BKD,\n",
    "        ) = ttnn.experimental.nlp_create_qkv_heads_decode(\n",
    "            xqkv_fused,\n",
    "            num_heads=self.n_local_heads,\n",
    "            num_kv_heads=self.n_local_kv_heads,\n",
    "            memory_config=ttnn.L1_HEIGHT_SHARDED_MEMORY_CONFIG,\n",
    "        )\n",
    "\n",
    "        ttnn.deallocate(xqkv_fused)\n",
    "\n",
    "        q_heads_1BQD = ttnn.linear(\n",
    "            q_heads_pre_rot_1BQD,\n",
    "            rot_mat,\n",
    "            program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "                q_heads_pre_rot_1BQD.shape[-2], q_heads_pre_rot_1BQD.shape[-1], rot_mat.shape[-1]\n",
    "            ),\n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "\n",
    "        k_heads_1BKD = ttnn.linear(\n",
    "            k_heads_pre_rot_1BKD,\n",
    "            rot_mat,\n",
    "            program_config=self.model_config[\"ROT_MAT_BMM_PROGCFG\"](\n",
    "                k_heads_pre_rot_1BKD.shape[-2], k_heads_pre_rot_1BKD.shape[-1], rot_mat.shape[-1]\n",
    "            ),\n",
    "            memory_config=k_heads_pre_rot_1BKD.memory_config(),\n",
    "            compute_kernel_config=self.compute_kernel_config_hifi2,\n",
    "            dtype=ttnn.bfloat16,\n",
    "        )\n",
    "\n",
    "        ttnn.deallocate(q_heads_pre_rot_1BQD)\n",
    "        ttnn.deallocate(k_heads_pre_rot_1BKD)\n",
    "\n",
    "        ###\n",
    "        # KV update\n",
    "        ###\n",
    "        keys = self.layer_past[0]\n",
    "        values = self.layer_past[1]\n",
    "\n",
    "        # k_heads, [seqlen, n_kv_heads, bsz, head_dim]\n",
    "        # v_heads [seqlen, n_kv_heads, bsz, head_dim]\n",
    "        # keys, [max_batch_size, n_kv_heads // configuration.num_devices, sliding_window, head_dim]\n",
    "        ttnn.experimental.paged_update_cache(\n",
    "            keys, k_heads_1BKD, update_idxs_tensor=current_pos, page_table=page_table)\n",
    "        \n",
    "        ttnn.experimental.paged_update_cache(\n",
    "            values, v_heads_1BKD, update_idxs_tensor=current_pos, page_table=page_table)\n",
    "        \n",
    "        self.layer_past[0] = keys\n",
    "        self.layer_past[1] = values\n",
    "\n",
    "        ttnn.deallocate(k_heads_1BKD)\n",
    "        ttnn.deallocate(v_heads_1BKD)\n",
    "\n",
    "        if page_table:\n",
    "            attn_output_1G4D = ttnn.transformer.paged_scaled_dot_product_attention_decode(\n",
    "                q_heads_1BQD,\n",
    "                keys,\n",
    "                values,\n",
    "                cur_pos_tensor=current_pos,\n",
    "                page_table_tensor=page_table,\n",
    "                scale=self.scale,\n",
    "                program_config=self.model_config[\"SDPA_DECODE_PROGCFG\"],\n",
    "                compute_kernel_config=self.model_config[\"SDPA_DECODE_COMPUTE_PROGCFG\"],\n",
    "                memory_config=ttnn.DRAM_MEMORY_CONFIG,\n",
    "            )\n",
    "        else:\n",
    "            attn_output_1G4D = ttnn.transformer.scaled_dot_product_attention_decode(\n",
    "                q_heads_1BQD,\n",
    "                keys,\n",
    "                values,\n",
    "                cur_pos_tensor=current_pos,\n",
    "                scale=self.scale,\n",
    "                program_config=self.model_config[\"SDPA_DECODE_PROGCFG\"],\n",
    "                compute_kernel_config=self.model_config[\"SDPA_DECODE_COMPUTE_PROGCFG\"],\n",
    "                memory_config=ttnn.DRAM_MEMORY_CONFIG,  # FIXME: why not L1 height sharded e.g. SCORES_BATCHED_MM_OUTPUT_MEMCFG?\n",
    "            )\n",
    "\n",
    "        ttnn.deallocate(q_heads_1BQD)\n",
    "\n",
    "        attn_output_11BH = ttnn.to_memory_config(\n",
    "            attn_output_1G4D, memory_config=self.model_config[\"SCORES_BATCHED_MM_OUTPUT_MEMCFG\"]\n",
    "        )\n",
    "        attn_output_cat = ttnn.experimental.nlp_concat_heads_decode(\n",
    "            attn_output_11BH,\n",
    "            num_heads=self.n_local_heads,\n",
    "        )\n",
    "        ttnn.deallocate(attn_output_11BH)\n",
    "        ttnn.deallocate(attn_output_1G4D)\n",
    "\n",
    "        return attn_output_cat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
