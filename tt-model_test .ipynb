{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 18:01:21.680 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2025-02-04 18:01:21.747 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2025-02-04 18:01:21.748 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2025-02-04 18:01:21.749 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-04 18:01:21.751 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2025-02-04 18:01:21.752 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2025-02-04 18:01:21.753 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2025-02-04 18:01:21.753 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2025-02-04 18:01:21.754 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2025-02-04 18:01:21.755 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2025-02-04 18:01:21.756 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2025-02-04 18:01:21.757 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2025-02-04 18:01:21.760 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2025-02-04 18:01:21.761 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2025-02-04 18:01:21.762 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2025-02-04 18:01:21.762 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2025-02-04 18:01:21.766 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv_transpose2d be migrated to C++?\n",
      "2025-02-04 18:01:21.769 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2025-02-04 18:01:21.770 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a5ffb42f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "import random\n",
    "from models.utility_functions import comp_pcc\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = {\n",
    "    \"Attention\": True,\n",
    "    \"RMSNorm\": True,\n",
    "    \"FeedForward\": True,\n",
    "    \"TransformerBlock\": True,\n",
    "    \"Transformer\": True,\n",
    "    \"Generation\": True\n",
    "}\n",
    "TEST_LAYER_NUM = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2025-02-04 18:01:21.894\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0, IOMMU: disabled\n",
      "\u001b[32m2025-02-04 18:01:21.895\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-02-04 18:01:21.895\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "# device.enable_program_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=64, n_layers=5, n_heads=8, n_kv_heads=4, vocab_size=512, hidden_dim=None, multiple_of=4, norm_eps=1e-05, max_seq_len=512, dropout=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint dict\n",
    "checkpoint_dict = torch.load(\"llama2/configs/stories260K.pth\")\n",
    "model_args = checkpoint_dict['model_args']\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "args = ModelArgs()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokens_num = args.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.model import precompute_freqs_cis\n",
    "torch_freqs_cos, torch_freqs_sin = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "freqs_cos = torch_freqs_cos.bfloat16()\n",
    "freqs_sin = torch_freqs_sin.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_printoptions(linewidth=100, precision=3,profile='full')\n",
    "torch.set_printoptions(profile='short')\n",
    "def check_close(torch_tensor, ttnn_tensor, atol=0.01):\n",
    "    ttnn_tensor = torch.Tensor(ttnn.to_torch(ttnn_tensor))\n",
    "    equals = torch.sum(torch.isclose(torch_tensor, ttnn_tensor, atol=atol))\n",
    "    perc = equals/torch_tensor.numel() * 100\n",
    "    print(f\"Close values: {perc:.3f}% ({torch_tensor.numel()})\")\n",
    "    print(f\"PCC: {comp_pcc(torch_tensor, ttnn_tensor)[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention init:\n",
      "ttnn: 0.016\n",
      "Attention forward:\n",
      "1°: 0.020\n",
      "2°: 0.045\n",
      "3°: 0.014\n",
      "4°: 0.047\n",
      "ttnn: 0.126\n",
      "torch: 0.039\n",
      "Close values: 89.059% (32768)\n",
      "PCC: 0.9998018312999007\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import Attention as torchAttention\n",
    "from ttllama2 import Attention\n",
    "\n",
    "if TEST['Attention']:\n",
    "    # ttnn.close_device(device)\n",
    "    # device = ttnn.open_device(device_id=device_id)\n",
    "    # ttnn.enable_cache(device)\n",
    "    x_torch = torch.rand((batch_size, tokens_num, args.dim), dtype=torch.bfloat16) \n",
    "\n",
    "    x = ttnn.from_torch(\n",
    "        x_torch,\n",
    "        layout=ttnn.TILE_LAYOUT, \n",
    "        device=device,\n",
    "        dtype=ttnn.bfloat16\n",
    "    )\n",
    "\n",
    "    print(\"Attention init:\")\n",
    "    start = time.time()\n",
    "    attention = Attention(args, state_dict, TEST_LAYER_NUM, device)\n",
    "    print(f\"ttnn: {time.time() - start:.3f}\")\n",
    "\n",
    "    print(\"Attention forward:\")\n",
    "    \n",
    "    start = time.time()\n",
    "    o_attention = attention.forward(x, freqs_cos, freqs_sin)\n",
    "    print(f\"ttnn: {time.time() - start:.3f}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    torch_attention = torchAttention(args, state_dict, TEST_LAYER_NUM)\n",
    "    print(f\"torch: {time.time() - start:.3f}\")\n",
    "\n",
    "    to_attention = torch_attention.forward(x_torch, torch_freqs_cos, torch_freqs_sin)\n",
    "    \n",
    "    check_close(to_attention, o_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare FeedForward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "Transformer:\n",
      "ttnn: 0.014\n",
      "torch: 0.013\n",
      "Close values: 37.268% (32768)\n",
      "PCC: 0.9997631032750476\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import FeedForward as torchFeedForward\n",
    "from ttllama2 import FeedForward\n",
    "\n",
    "if TEST['FeedForward']:\n",
    "\n",
    "    ttnn.close_device(device)\n",
    "    device = ttnn.open_device(device_id=device_id)\n",
    "    x_torch = torch.rand((batch_size, tokens_num, args.dim), dtype=torch.bfloat16) \n",
    "    x = ttnn.from_torch(\n",
    "        x_torch,\n",
    "        layout=ttnn.TILE_LAYOUT, \n",
    "        device=device,\n",
    "        dtype=ttnn.bfloat16\n",
    "    )\n",
    "    tt_ff = FeedForward(args.dim, args.hidden_dim, args.multiple_of, TEST_LAYER_NUM, state_dict, device)\n",
    "    torch_ff = torchFeedForward(args.dim, args.hidden_dim, args.multiple_of, 0, state_dict, TEST_LAYER_NUM)\n",
    "    \n",
    "    start = time.time()\n",
    "    o_ff = tt_ff.forward(x)\n",
    "    print(\"Transformer:\")\n",
    "    print(f\"ttnn: {time.time() - start:.3f}\")\n",
    "    start = time.time()\n",
    "    to_ff = torch_ff.forward(x_torch)\n",
    "    print(f\"torch: {time.time() - start:.3f}\")\n",
    "\n",
    "    check_close(to_ff, o_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Transformer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "TransformerBlock:\n",
      "ttnn: 2.479\n",
      "torch: 0.048\n",
      "Close values: 22.971% (32768)\n",
      "PCC: 0.9992009902669368\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import TransformerBlock as torchTransformerBlock\n",
    "from ttllama2 import TransformerBlock \n",
    "\n",
    "\n",
    "if TEST['TransformerBlock']:\n",
    "    ttnn.close_device(device)\n",
    "    device = ttnn.open_device(device_id=device_id)\n",
    "    x_torch = torch.rand((batch_size, tokens_num, args.dim), dtype=torch.bfloat16)\n",
    "        \n",
    "    x = ttnn.from_torch(\n",
    "        x_torch,\n",
    "        device=device,\n",
    "        dtype=ttnn.bfloat16\n",
    "    )\n",
    "    x = ttnn.to_layout(x, ttnn.TILE_LAYOUT)\n",
    "\n",
    "    tt_tb = TransformerBlock(TEST_LAYER_NUM, args, state_dict, device)\n",
    "    torch_tb = torchTransformerBlock(TEST_LAYER_NUM, args, state_dict)\n",
    "    _, seq_len, _ = x_torch.shape\n",
    "\n",
    "    start = time.time()\n",
    "    o_tb = tt_tb.forward(x, freqs_cos[:seq_len], freqs_sin[:seq_len])\n",
    "    print(\"TransformerBlock:\")\n",
    "    print(f\"ttnn: {time.time() - start:.3f}\")\n",
    "    start = time.time()\n",
    "    to_tb = torch_tb.forward(x_torch, freqs_cos[:seq_len], freqs_sin[:seq_len])\n",
    "    print(f\"torch: {time.time() - start:.3f}\")\n",
    "    check_close(to_tb.bfloat16(), o_tb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close values drops hard here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "Transformer:\n",
      "ttnn: 0.820\n",
      "torch: 0.058\n",
      "Close values: 2.344% (512)\n",
      "PCC: 0.9999211092957268\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import Transformer as torchTransformer\n",
    "from ttllama2 import Transformer\n",
    "from llama2.tokenizer import Tokenizer\n",
    "\n",
    "if TEST['Transformer']:\n",
    "    ttnn.close_device(device)\n",
    "    device = ttnn.open_device(device_id=device_id)\n",
    "    \n",
    "    x_torch = torch.randint(1, args.vocab_size-1, (1, 152))\n",
    "    seq_len = 1\n",
    "\n",
    "    x = ttnn.from_torch(x_torch, device=device)\n",
    "    x = ttnn.to_layout(x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    \n",
    "    tt_trans = Transformer(args, state_dict, device)\n",
    "    torch_trans = torchTransformer(args, state_dict)\n",
    "    start = time.time()\n",
    "    o_trans = tt_trans.forward(x)\n",
    "    print(\"Transformer:\")\n",
    "    print(f\"ttnn: {time.time() - start:.3f}\")\n",
    "    start = time.time()\n",
    "    to_trans = torch_trans.forward(x_torch)\n",
    "    print(f\"torch: {time.time() - start:.3f}\")\n",
    "\n",
    "    check_close(to_trans.bfloat16(), o_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation\n",
    "#### WIP: Speaking really bad, need to adjust something somewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(logits, temperature, top_k):\n",
    "    logits = logits[:, -1, :] # crop to just the final time step\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        # \"sample\" the single most likely index\n",
    "        _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "    else:\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = torch.functional.F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.tokenizer import Tokenizer\n",
    "\n",
    "enc = Tokenizer(tokenizel_path=\"./llama2/tokenizer.model\")\n",
    "temperature = 1.0\n",
    "top_k = 300\n",
    "\n",
    "start = \"Dream comes true!\"\n",
    "start_ids = enc.encode(start, bos=True, eos=False)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dream comes true! Daddy was very excited. He explained it was something yummy.\n",
      "One day, Dizzy found a stubborn box on a hole. It was coming from the box and Benny was helpless. Don't worry,\n"
     ]
    }
   ],
   "source": [
    "torch_trans = torchTransformer(args, state_dict)\n",
    "\n",
    "# [1,13]\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "for _ in range(100):\n",
    "    # if the sequence context is growing too long we must crop it at block_size\n",
    "    # x = x if idx.size(1) <= args.max_seq_len else idx[:, -args.max_seq_len:]\n",
    "    \n",
    "    # forward the model to get the logits for the index in the sequence\n",
    "    logits = torch_trans.forward(x)\n",
    "\n",
    "    idx_next = get_next_token(logits, temperature, top_k)\n",
    "    # append sampled index to the running sequence and continue\n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "\n",
    "\n",
    "print(enc.decode(x[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Enabling program cache on device 0\n",
      "Dream comes true! \"edadadit ofout herenaesestb LilyeflfendJeliforon niin linll ankow brieiverke itedlMeom hisveldganIy akireherarotayitan Heendll are” butadltilannstoanv time.or ofs. toomary”Blenot\n",
      "Execution time: 14.889288902282715\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "# device.enable_program_cache()\n",
    "\n",
    "tt_trans = Transformer(args, state_dict, device)\n",
    "\n",
    "x = (torch.tensor(start_ids, dtype=torch.long)[None, ...])\n",
    "\n",
    "time_forward = 0\n",
    "for _ in range(100):\n",
    "    tt_x = ttnn.from_torch(x, device=device)\n",
    "    tt_x = ttnn.to_layout(tt_x, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "    start = time.time()\n",
    "    logits = tt_trans.forward(tt_x)\n",
    "    time_forward += time.time() - start\n",
    "\n",
    "    logits = ttnn.to_torch(logits)\n",
    "    idx_next = get_next_token(logits, temperature, top_k)    \n",
    "    x = torch.cat((x, idx_next), dim=1)\n",
    "\n",
    "\n",
    "print(enc.decode(x[0].tolist()))\n",
    "\n",
    "print(f\"Execution time: {time_forward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
