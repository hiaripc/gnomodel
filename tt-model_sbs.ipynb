{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 17:17:06.072 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-11 17:17:06.132 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-11 17:17:06.133 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-11 17:17:06.134 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 17:17:06.135 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-11 17:17:06.136 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-11 17:17:06.136 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-11 17:17:06.137 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-11 17:17:06.137 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-11 17:17:06.138 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-11 17:17:06.139 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-11 17:17:06.139 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-11 17:17:06.139 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-11 17:17:06.140 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-11 17:17:06.140 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-11 17:17:06.142 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-11 17:17:06.147 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-11 17:17:06.148 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import ttnn\n",
    "from llama2.model import ModelArgs\n",
    "from typing import Tuple\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2024-12-11 17:17:06.186\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-11 17:17:06.186\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-11 17:17:06.186\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 1 PCI device : [0]\n",
      "\u001b[32m2024-12-11 17:17:06.187\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 64, 'n_layers': 5, 'n_heads': 8, 'n_kv_heads': 4, 'vocab_size': 512, 'multiple_of': 4, 'max_seq_len': 512, 'dropout': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint dict\n",
    "import os\n",
    "checkpoint_dict = torch.load(\"llama2/configs/stories260K.pth\")\n",
    "model_args = checkpoint_dict['model_args']\n",
    "print(model_args)\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "args = ModelArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "tokens_num = args.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2500, 0.5000, 0.7500])\n",
      "10000.0\n"
     ]
    }
   ],
   "source": [
    "from llama2.model import precompute_freqs_cis\n",
    "torch_freqs_cos, torch_freqs_sin = precompute_freqs_cis(args.dim // args.n_heads, args.max_seq_len)\n",
    "# freqs_cos = ttnn.from_torch(torch_freqs_cos, device=device)\n",
    "#freqs_sin = ttnn.from_torch(torch_freqs_sin, device=device)\n",
    "freqs_cos = torch_freqs_cos\n",
    "freqs_sin = torch_freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: ttnn.Tensor, x: ttnn.Tensor):\n",
    "    ndim = len(x.shape)\n",
    "    assert 0 <= 1 < ndim\n",
    "    print(f\"{freqs_cis.shape}, {x.shape[1]},{x.shape[-1]}\")\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]) \n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    print(freqs_cis.shape)\n",
    "    print(shape)\n",
    "    return ttnn.reshape(freqs_cis, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: ttnn.Tensor,\n",
    "    xk: ttnn.Tensor,\n",
    "    freqs_cos: ttnn.Tensor,\n",
    "    freqs_sin: ttnn.Tensor\n",
    ") -> Tuple[ttnn.Tensor, ttnn.Tensor]:\n",
    "    print(\"xq:\", xq.shape)\n",
    "    assert tuple(xq.shape)[0] == 1, \"Only works with batch 1 :-C\"\n",
    "    xq = ttnn.reshape(xq, (tuple(xq.shape)[:-1] + (-1,2)))\n",
    "    # Cannot unbind, cannot slice with [:..], must use ttnn.slice\n",
    "    # xq_r, xq_i = xq.unbind(-1)\n",
    "    # Squeeze because to_layour only supports 4D max tensor\n",
    "    xq = ttnn.squeeze(xq, 0)\n",
    "    xq = ttnn.to_layout(xq, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xq = ttnn.unsqueeze(xq, 0)\n",
    "    xq_r = ttnn.slice(xq, [0,0,0,0,0], list(tuple(xq.shape)[:-1] + (1,)))\n",
    "    xq_r = ttnn.squeeze(xq_r, -1)\n",
    "    print(\"xq_r:\", xq_r.shape)\n",
    "    # ttnn.deallocate(xq_r)\n",
    "    xq_i = ttnn.slice(xq, [0,0,0,0,1], list(tuple(xq.shape)[:-1] + (2,)))\n",
    "    xq_i = ttnn.squeeze(xq_i, -1)    \n",
    "    print(\"xq_i:\", xq_i.shape)\n",
    "\n",
    "    xk = ttnn.squeeze(xk, 0)\n",
    "    xk = ttnn.to_layout(xk, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xk = ttnn.unsqueeze(xk, 0)\n",
    "    xk = ttnn.unsqueeze(xk, 0)\n",
    "    xk_r = ttnn.slice(xk, [0,0,0,0,0],tuple(xk.shape)[:-1] + (1,))\n",
    "    xk_r = ttnn.squeeze(xk_r, -1)\n",
    "    # ttnn.deallocate(xk_r)\n",
    "    xk_i = ttnn.slice(xk, [0,0,0,0,1], tuple(xk.shape)[:-1] + (2,))\n",
    "    xk_i = ttnn.squeeze(xk_i, -1)  \n",
    "\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    freqs_sin = ttnn.to_layout(freqs_sin, layout = ttnn.TILE_LAYOUT)\n",
    "    freqs_cos = ttnn.to_layout(freqs_cos, layout = ttnn.TILE_LAYOUT)\n",
    "\n",
    "    # apply rotation using real numbers\n",
    "    xq_r = ttnn.to_layout(xq_r, layout = ttnn.TILE_LAYOUT)\n",
    "    xq_i = ttnn.to_layout(xq_i, layout = ttnn.TILE_LAYOUT)\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "\n",
    "    xk_r = ttnn.to_layout(xk_r, layout = ttnn.TILE_LAYOUT)\n",
    "    xk_i = ttnn.to_layout(xk_i, layout = ttnn.TILE_LAYOUT)\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "    \n",
    "    # there's no ttnn.stack nor ttnn.flatten :-)\n",
    "    xq_out_r = ttnn.to_layout(xq_out_r, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    xq_out_i = ttnn.to_layout(xq_out_i, layout = ttnn.ROW_MAJOR_LAYOUT)\n",
    "    print(xq_out_r.shape, xq_out_i.shape)\n",
    "    # Create new dimension\n",
    "    xq_out_r = ttnn.unsqueeze(xq_out_r, -1)\n",
    "    # Concatenate along the new dimension\n",
    "    xq_out = torch.concatenate([xq_out_r, xq_out_i], dim=-1)\n",
    "    # todo: implement flatten\n",
    "    print(xq_out.shape)\n",
    "\n",
    "    # xq_out = ttnn.flatten(xq_out, 3)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2.model import apply_rotary_emb as apply_rotary_emb_torch\n",
    "\n",
    "def apply_rotary_emb_host(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq = ttnn.to_torch(xq)\n",
    "    xk = ttnn.to_torch(xk) \n",
    "    xq_out, xk_out = apply_rotary_emb_torch(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "    return (\n",
    "        ttnn.from_torch(xq_out, device=device, layout=ttnn.TILE_LAYOUT, dtype=ttnn.bfloat16), \n",
    "        ttnn.from_torch(xk_out, device=device, layout=ttnn.TILE_LAYOUT, dtype=ttnn.bfloat16)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: ttnn.Tensor, n_rep: int) -> ttnn.Tensor:\n",
    "    return ttnn.repeat_interleave(x, dim=2, repeats=n_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(LightweightModule):\n",
    "    def __init__(self, args: ModelArgs, state_dict: dict, layer_num, device):\n",
    "        super().__init__()\n",
    "        self.state_dict = state_dict\n",
    "        self.max_batch_size = 1\n",
    "        self.device = device\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_q_heads = args.n_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads        \n",
    "        # Indicates how many times the Keys and Values should be repeated        \n",
    "        self.n_rep = args.n_heads // self.n_kv_heads\n",
    "        \n",
    "        prefix = f\"layers.{layer_num}.attention.\"\n",
    "\n",
    "        self.wq = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wq.weight\"], -2, -1,),\n",
    "            layout=ttnn.TILE_LAYOUT,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.wk = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wk.weight\"], -2, -1,),\n",
    "            layout=ttnn.TILE_LAYOUT,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.wv = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wv.weight\"], -2, -1,),\n",
    "            layout=ttnn.TILE_LAYOUT,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.wo = ttnn.as_tensor(\n",
    "            torch.transpose(self.state_dict[f\"{prefix}wo.weight\"], -2, -1,),\n",
    "            layout=ttnn.TILE_LAYOUT,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "        self.mask = torch.triu(mask, diagonal=1)\n",
    "        self.mask = ttnn.from_torch(self.mask, device=device, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "\n",
    "    def forward_opt(self, x: ttnn.Tensor, freqs_cos:torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        ... \n",
    "\n",
    "    def forward(self, x: ttnn.Tensor, freqs_cos:torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq = ttnn.linear(\n",
    "            x,\n",
    "            self.wq,\n",
    "            bias=None,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "        )\n",
    "        xq = ttnn.to_layout(xq, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "        xq = ttnn.reshape(xq, (bsz, seqlen, self.n_q_heads, self.head_dim))\n",
    "        xq = ttnn.to_layout(xq, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "\n",
    "        xk = ttnn.linear(\n",
    "            x,\n",
    "            self.wk,\n",
    "            bias=None,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "        )\n",
    "        xk = ttnn.to_layout(xk, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "        xk = ttnn.reshape(xk, (bsz, seqlen, self.n_kv_heads, self.head_dim))\n",
    "        xk = ttnn.to_layout(xk, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "\n",
    "        xv = ttnn.linear(\n",
    "            x,\n",
    "            self.wv,\n",
    "            bias=None,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "        )\n",
    "        xv = ttnn.to_layout(xv, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "        xv = ttnn.reshape(xv, (bsz, seqlen, self.n_kv_heads, self.head_dim))\n",
    "        xv = ttnn.to_layout(xv, layout=ttnn.TILE_LAYOUT)\n",
    "        print(xq.get_dtype(), xk.get_dtype(), xv.get_dtype())\n",
    "        \n",
    "        # Apply RoPE\n",
    "        xq, xk = apply_rotary_emb_host(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # premute instead of transpose \n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = ttnn.permute(xq, (0, 2, 1, 3))\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        xk = ttnn.permute(xk, (0, 2, 1, 3))\n",
    "        xv = ttnn.permute(xv, (0, 2, 1, 3))\n",
    "\n",
    "        xq = ttnn.to_layout(\n",
    "            xq, \n",
    "            layout=ttnn.TILE_LAYOUT, \n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "        xk = ttnn.to_layout(\n",
    "            xk, \n",
    "            layout=ttnn.TILE_LAYOUT, \n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "        xv = ttnn.to_layout(\n",
    "            xv, \n",
    "            layout=ttnn.TILE_LAYOUT, \n",
    "            memory_config=ttnn.DRAM_MEMORY_CONFIG)\n",
    "\n",
    "        print(xq.shape, xk.shape, xv.shape)\n",
    "        # use flash attention, shape problem\n",
    "        # xk = ttnn.permute(xk, (0, 1, 3, 2))\n",
    "        if False:\n",
    "            output = ttnn.transformer.scaled_dot_product_attention(\n",
    "                xq, \n",
    "                xk, \n",
    "                xv, \n",
    "                attn_mask=None, \n",
    "                is_causal=True\n",
    "            )\n",
    "        \n",
    "        attention_scores = ttnn.matmul(\n",
    "            xq,\n",
    "            ttnn.permute(xk, (0, 1, 3, 2)),\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat16,\n",
    "            # core_grid=ttnn.CoreGrid(y=batch_size, x=num_cores_x),\n",
    "        )\n",
    "\n",
    "        attention_scores = ttnn.div(attention_scores, math.sqrt(self.head_dim))\n",
    "        attention_scores = attention_scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "\n",
    "        attention_scores = ttnn.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        output = ttnn.matmul(\n",
    "            attention_scores,\n",
    "            xv,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat16\n",
    "        )\n",
    "\n",
    "        output = ttnn.permute(output, (0, 2, 1, 3))\n",
    "        output = ttnn.to_layout(output, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "        output = ttnn.reshape(output, (bsz, seqlen, -1))\n",
    "        output = ttnn.to_layout(output, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "        output = ttnn.linear(\n",
    "            output,\n",
    "            self.wo,\n",
    "            bias=None,\n",
    "            memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "            dtype=ttnn.bfloat8_b,\n",
    "        )\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "DataType.BFLOAT16 DataType.BFLOAT16 DataType.BFLOAT16\n",
      "xq: torch.Size([1, 512, 8, 8])\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "xq_reshaped: torch.Size([1, 512, 8, 4, 2])\n",
      "xq_r: torch.Size([1, 512, 8, 4])\n",
      "torch.Size([1, 512, 8, 4]) torch.Size([1, 512, 8, 4])\n",
      "1: torch.Size([1, 512, 8, 4, 2])\n",
      "2: torch.Size([1, 512, 8, 8])\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "ttnn.Shape([1, 8, 512, 8[32]]) ttnn.Shape([1, 8, 512, 8[32]]) ttnn.Shape([1, 8, 512, 8[32]])\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n"
     ]
    }
   ],
   "source": [
    "layer_num = 0\n",
    "attention = Attention(args, state_dict, layer_num, device)\n",
    "# attention(ttnn.from_torch(torch.rand((1,64)), layout=ttnn.TILE_LAYOUT), 1)\n",
    "\n",
    "x_torch = torch.rand((batch_size,tokens_num,args.dim)) \n",
    "\n",
    "x = ttnn.from_torch(\n",
    "    x_torch,\n",
    "    layout=ttnn.TILE_LAYOUT, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "output = attention.forward(x, freqs_cos, freqs_sin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
