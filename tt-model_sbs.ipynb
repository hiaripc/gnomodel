{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 17:42:20.514 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/bach/.cache/ttnn,model_cache_path=/home/bach/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-12-09 17:42:20.572 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-12-09 17:42:20.573 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-12-09 17:42:20.574 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-09 17:42:20.575 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-12-09 17:42:20.576 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-12-09 17:42:20.576 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-12-09 17:42:20.577 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.to_device be migrated to C++?\n",
      "2024-12-09 17:42:20.578 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.from_device be migrated to C++?\n",
      "2024-12-09 17:42:20.579 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-12-09 17:42:20.579 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-12-09 17:42:20.580 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-12-09 17:42:20.580 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-12-09 17:42:20.581 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-12-09 17:42:20.581 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-12-09 17:42:20.581 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-12-09 17:42:20.587 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-12-09 17:42:20.587 | WARNING  | ttnn.decorators:operation_decorator:801 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from model import ModelArgs\n",
    "from typing import Tuple\n",
    "\n",
    "# just a super simple forward without host overhead\n",
    "from ttcode.lightweightmodule import LightweightModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the TinyStories 260k model\n",
    "    dim: int = 64\n",
    "    n_layers: int = 5\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: Optional[int] = 4\n",
    "    vocab_size: int = 512\n",
    "    hidden_dim: Optional[int] = None\n",
    "    multiple_of: int = 4  # MLP hidden layer size will be multiple of\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 512\n",
    "    dropout: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Norm imported from a common\n",
    "from ttcode import rmsnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\n",
      "\u001b[32m2024-12-09 17:42:20.655\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-09 17:42:20.655\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Unknown board type for chip 0. This might happen because chip is running old firmware. Defaulting to DEFAULT\n",
      "\u001b[32m2024-12-09 17:42:20.656\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 1 PCI device : [0]\n",
      "\u001b[32m2024-12-09 17:42:20.656\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.30.0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1000 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Profiler started on device 0\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n",
      "\u001b[38;2;000;128;000m                 Always\u001b[0m | \u001b[1m\u001b[38;2;255;165;000mWARNING \u001b[0m | Specifying tile shape for a row major layout is deprecated, and will be removed soon\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'float' and 'ttnn._ttnn.tensor.Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xq_out\u001b[38;5;241m.\u001b[39mtype_as(xq), xk_out\u001b[38;5;241m.\u001b[39mtype_as(xk)\n\u001b[1;32m     64\u001b[0m device \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mopen_device(device_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mprecompute_freqs_cis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mprecompute_freqs_cis\u001b[0;34m(dim, end, theta, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m t \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mfrom_torch(a, dtype\u001b[38;5;241m=\u001b[39mttnn\u001b[38;5;241m.\u001b[39mbfloat16, layout\u001b[38;5;241m=\u001b[39mttnn\u001b[38;5;241m.\u001b[39mTILE_LAYOUT, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m e \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mdiv(t, dim)\n\u001b[0;32m---> 10\u001b[0m freqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[43mtheta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Construct the positions (the \"m\" parameter)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Shape: (Seq_Len)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39marange(end, device\u001b[38;5;241m=\u001b[39mfreqs\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'float' and 'ttnn._ttnn.tensor.Tensor'"
     ]
    }
   ],
   "source": [
    "# RoPE implementation\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, device=0):\n",
    "    # Build the theta parameter\n",
    "    # According to the formula theta_i = 10000^(-2(i-1)/dim) for i = [1, 2, ... dim/2]\n",
    "    # Shape: (Head_Dim / 2)\n",
    "    # freqs = 1.0 / (theta ** (ttnn.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    a = torch.arange(0, dim, 2)[: (dim // 2)]\n",
    "    t = ttnn.from_torch(a, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "    e = ttnn.div(t, dim)\n",
    "    # freqs = 1.0 / (theta ** e)\n",
    "    freqs = 1.0 / (theta ** e)\n",
    "    \n",
    "    # Construct the positions (the \"m\" parameter)\n",
    "    # Shape: (Seq_Len)\n",
    "    t = ttnn.arange(end, device=freqs.device)  # type: ignore\n",
    "    # Multiply each theta by each position using the outer product.\n",
    "    # Shape: (Seq_Len) outer_product* (Head_Dim / 2) -> (Seq_Len, Head_Dim / 2)\n",
    "    # freqs = ttnn.outer(t, freqs).float()  # type: ignore\n",
    "    freqs = ttnn.outer(t, freqs)  # type: ignore\n",
    "    \n",
    "    # We can computer the cos and sin\n",
    "    # Shape: (Seq_len, Head_Dim / 2)\n",
    "    freqs_cos = ttnn.cos(freqs)  # real part\n",
    "    freqs_sin = ttnn.sin(freqs)  # imaginary part\n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: ttnn.Tensor, x: ttnn.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: ttnn.Tensor,\n",
    "    xk: ttnn.Tensor,\n",
    "    freqs_cos: ttnn.Tensor,\n",
    "    freqs_sin: ttnn.Tensor\n",
    ") -> Tuple[ttnn.Tensor, ttnn.Tensor]:\n",
    "\n",
    "    # reshape xq and xk to match the complex representation\n",
    "    # xq.shape[:-1] + (-1, 2) -> tolgo ultima dim, e faccio reshape che abbia come ultima dim 2\n",
    "    # il -1 è per fare il modo che sia compatibile: (5,1,8) -> .reshape(5,1,-1,2) -> (5,1,4,2) \n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "\n",
    "    # reshape freqs_cos and freqs_sin for broadcasting\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    # apply rotation using real numbers\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    # flatten last two dimensions\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "device = ttnn.open_device(device_id=0)\n",
    "precompute_freqs_cis(4, 4, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt-menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
